{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false \n",
    "#| output: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import os \n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "import jax.numpy as np  # import here so that any warnings about no GPU are not shown in website.\n",
    "np.arange(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalizing to 2D\n",
    "\n",
    "Thus far, we've explored score models in the context of 1D data.\n",
    "This is intentional!\n",
    "By working out the core ideas in a single dimension,\n",
    "we can more easily reason about what actually is happening --\n",
    "humans are, after all, very good at thinking in 1D.\n",
    "In effect, we eliminate the cognitive load that comes with thinking multi-dimensionally.\n",
    "Through this, the framework of how to think about \n",
    "how to use score models to generate data is quite clear.\n",
    "Our ingredients are:\n",
    "\n",
    "- Data,\n",
    "- A trainable model that can approximate the score of our data (implying that yes, we will train that model!), and\n",
    "- A procedure for noising up data and reversing that process to re-generate new data.\n",
    "\n",
    "Alas, however, the world of data that inhabits our world is rarely just 1D.\n",
    "More often than not, the data that we will encounter is going to be multi-dimensional.\n",
    "To exacerbate the matter, our data are also oftentimes discrete and not continuous,\n",
    "such as text, protein sequences, and more.\n",
    "Do the ideas explored in 1D generalize to multiple dimensions?[^1]\n",
    "In this notebook, I want to show how we can generalize from 1D to 2D.\n",
    "(With a bit of hand-waving,\n",
    "I'll claim at the end that this all works in n-dimensions too!)\n",
    "\n",
    "[^1]: Of course, yes -- this is a rhetorical question --\n",
    "and the more important point here is figuring out \n",
    "what we need to do to generalize beyond 1D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data: Half Moons\n",
    "\n",
    "As our anchoring example, we will use the half-moons dataset from `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| fig-cap: Half moons dataset.\n",
    "#| label: fig-moons\n",
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "X, y = make_moons(n_samples=1000, noise=0.1)\n",
    "fig, axes = plt.subplots(figsize=(4, 4))\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment goes here on labels. Though we have labels, we'll work with them later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we noise up the data.\n",
    "Strictly speaking with a constant drift term,\n",
    "we need only parameterize our diffusion term using `t` (time)\n",
    "and don't really need to use `diffrax`'s SDE capabilities.\n",
    "We can noise up data by applying a draw\n",
    "from an isotropic Gaussian with covariance equal to the time elapsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import vmap\n",
    "from functools import partial\n",
    "\n",
    "def noise_batch(key, X: np.ndarray, t: float) -> np.ndarray:\n",
    "    \"\"\"Noise up one batch of data.\n",
    "    \n",
    "    :param x: One batch of data.\n",
    "        Should be of shape (1, n_dims).\n",
    "    :param t: Time scale at which to noise up.\n",
    "    :returns: A NumPy array of noised up data.\n",
    "    \"\"\"\n",
    "    if t == 0.0:\n",
    "        return X\n",
    "    cov = np.eye(len(X)) * t\n",
    "    return X + random.multivariate_normal(key=key, mean=np.zeros(len(X)), cov=cov)\n",
    "\n",
    "\n",
    "def noise(key, X, t):\n",
    "    keys = random.split(key, num=len(X))\n",
    "    return vmap(partial(noise_batch, t=t))(keys, X)\n",
    "\n",
    "from jax import random \n",
    "\n",
    "fig, axes = plt.subplots(figsize=(8, 8), nrows=3, ncols=3)\n",
    "ts = np.linspace(0.001, 0.2, 9)\n",
    "key = random.PRNGKey(99)\n",
    "noise_level_keys = random.split(key, 9)\n",
    "noised_datas = []\n",
    "for t, ax, key in zip(ts, axes.flatten(), noise_level_keys):\n",
    "    noised_data = noise(key, X, t)\n",
    "    noised_datas.append(noised_data)\n",
    "    ax.scatter(noised_data[:, 0], noised_data[:, 1], alpha=0.1)\n",
    "    ax.set_title(f\"{t:.2f}\")\n",
    "noised_datas = np.stack(noised_datas)\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check `noised_data`'s shape, should be `(time, batch, n_data_dims)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noised_datas.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can set up a score model to be trained on each time point's noised-up data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "from jax import nn\n",
    "\n",
    "\n",
    "class ScoreModel2D(eqx.Module):\n",
    "    \"\"\"Time-dependent score model.\n",
    "\n",
    "    We choose an MLP here with 2 inputs (`x` and `t` concatenated),\n",
    "    and output a scalar which is the estimated score.\n",
    "    \"\"\"\n",
    "\n",
    "    mlp: eqx.Module\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_size=3,\n",
    "        out_size=2,\n",
    "        width_size=256,\n",
    "        depth=2,\n",
    "        activation=nn.softplus,\n",
    "        key=random.PRNGKey(45),\n",
    "    ):\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            in_size=in_size,\n",
    "            out_size=out_size,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            activation=activation,\n",
    "            key=key,\n",
    "        )\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x: np.array, t: float):\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        :param x: Data. Should be of shape (1, :),\n",
    "            as the model is intended to be vmapped over batches of data.\n",
    "        :returns: Estimated score of a Gaussian.\n",
    "        \"\"\"\n",
    "        t = np.array([t])\n",
    "        x = np.concatenate([x, t])\n",
    "        return self.mlp(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test that the model's forward pass works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from jax import vmap \n",
    "\n",
    "model = ScoreModel2D()\n",
    "t = 0.1\n",
    "\n",
    "X_noised = noise(key, X, t)\n",
    "out = vmap(partial(model, t=t))(X_noised)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jacfwd, jit \n",
    "\n",
    "def sde_score_matching_loss(model, noised_data: np.ndarray, t: float):\n",
    "    \"\"\"Score matching loss for SDE-based score models.\n",
    "    \n",
    "    :param model: Equinox model.\n",
    "    :param noised_data: Batch of data from 1 noise scale of shape (batch, n_data_dims).\n",
    "    :param t: Time in SDE at which the noise scale was evaluated.\n",
    "    \"\"\"\n",
    "    model = partial(model, t=t)\n",
    "    dmodel = jacfwd(model, argnums=0)\n",
    "    term1 = vmap(dmodel)(noised_data)\n",
    "    term1 = vmap(np.diagonal)(term1)\n",
    "    term2 = 0.5 * vmap(model)(data) ** 2\n",
    "    inner_term = term1 + term2\n",
    "    summed_by_dims = vmap(np.sum)(inner_term)\n",
    "    return np.mean(summed_by_dims)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def joint_sde_score_matching_loss(model, noised_data_all, ts):\n",
    "    \"\"\"Joint score matching loss.\n",
    "    \n",
    "    :param model: An equinox model.\n",
    "    :param noised_data_all: An array of shape (time, batch, n_data_dims).\n",
    "    :param ts: An array of shape (time,).\n",
    "    \"\"\"\n",
    "    loss = partial(sde_score_matching_loss, model)\n",
    "    losses = vmap(loss)(noised_data_all, ts)\n",
    "    return np.sum(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we calculate loss over all noised up data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments\n",
    "key = random.PRNGKey(55)\n",
    "model = ScoreModel2D()\n",
    "data = X\n",
    "\n",
    "# model\n",
    "joint_sde_score_matching_loss(model, noised_datas, ts=ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dloss = eqx.filter_value_and_grad(joint_sde_score_matching_loss)\n",
    "value, grads = dloss(model, noised_datas, ts=ts)\n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "model = ScoreModel2D()\n",
    "\n",
    "optimizer = optax.chain(\n",
    "    optax.adam(5e-2),\n",
    "    optax.clip(0.001),\n",
    ")\n",
    "\n",
    "opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
    "dloss = eqx.filter_value_and_grad(joint_sde_score_matching_loss)\n",
    "\n",
    "n_steps = 13_000\n",
    "iterator = tqdm(range(n_steps))\n",
    "loss_history = []\n",
    "key = random.PRNGKey(555)\n",
    "keys = random.split(key, n_steps)\n",
    "\n",
    "updated_score_model = model\n",
    "for step in iterator:\n",
    "    loss_score, grads = dloss(updated_score_model, noised_datas, ts)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    updated_score_model = eqx.apply_updates(updated_score_model, updates)\n",
    "    iterator.set_description(f\"Score: {loss_score}\")\n",
    "    loss_history.append(float(loss_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random, numpy as np, vmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we generate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models.utils import generate_mixture_2d\n",
    "\n",
    "key = random.PRNGKey(42)\n",
    "data, k3 = generate_mixture_2d(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data just to make sure we know what it's all about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.scatter(data[:, 0], data[:, 1], alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train a score model for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score of a 2D dataset is the gradient w.r.t. the inputs.\n",
    "Because the data are multi-dimensional,\n",
    "our gradients are necessarily equally dimensioned;\n",
    "they would be esssentially partial derivatives w.r.t. the input.\n",
    "Specifically, the score function maps $\\mathbb{R}^d \\rightarrow \\mathbb{R}^d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models.models import nn_model\n",
    "from score_models.losses import score_matching_loss\n",
    "from functools import partial\n",
    "from jaxopt import GradientDescent\n",
    "\n",
    "init_fun, nn_score_func = nn_model(output_dim=2)\n",
    "k4, k5 = random.split(k3)\n",
    "_, params_init = init_fun(k4, input_shape=(None, 2))\n",
    "\n",
    "# Test-drive forward pass\n",
    "out_test = vmap(partial(nn_score_func, params_init))(data)\n",
    "out_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging purposes\n",
    "out_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to write the score matching loss.\n",
    "The score matching loss is the sum over all dimensions\n",
    "of the mean over all samples,\n",
    "as given by equation 6 in the JMLR paper (2005) by Aapo Hyvärinen.\n",
    "In earlier experiments, I also observed exploding weights leading to NaN values,\n",
    "so I will be applying weight L2 regularization to prevent that from happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models.losses import score_matching_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit \n",
    "myloss = jit(partial(score_matching_loss, score_func=nn_score_func))\n",
    "solver = GradientDescent(fun=myloss, maxiter=10000)\n",
    "result = solver.run(params_init, batch=data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.tree_util import tree_flatten, tree_map\n",
    "\n",
    "params_flat, _ = tree_flatten(result.params)\n",
    "params_flat = tree_map(lambda x: x.flatten(), params_flat)\n",
    "params_flat = np.concatenate(params_flat)\n",
    "params_flat.max(), params_flat.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize learned gradient field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 21\n",
    "xs = np.linspace(-30, 30, n_points)\n",
    "ys = np.linspace(-30, 30, n_points)\n",
    "xxs, yys = np.meshgrid(xs, ys)\n",
    "xxs.shape, yys.shape\n",
    "\n",
    "x_y_pair = np.vstack([xxs.flatten(), yys.flatten()]).T\n",
    "x_y_pair.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "result.params\n",
    "gradient_field = vmap(partial(nn_score_func, result.params))(x_y_pair)\n",
    "\n",
    "for xy_pair, vect in zip(x_y_pair, gradient_field):\n",
    "    axes.arrow(*xy_pair, *vect * 0.1, width=0.3, alpha=0.1)    \n",
    "axes.scatter(*data.T, alpha=0.1, color=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we sample with Langevin Dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models.sampler import langevin_dynamics\n",
    "\n",
    "starter_xs = random.multivariate_normal(k5, mean=np.array([-5, -5]), cov=np.eye(2)*20, shape=(4000,)) \n",
    "epsilon = 5e-3\n",
    "starting_states, final_states, chain_samples = langevin_dynamics(\n",
    "    n_chains=4000, \n",
    "    n_samples=8000, \n",
    "    key=key, \n",
    "    epsilon=epsilon, \n",
    "    score_func=nn_score_func, \n",
    "    params=result.params, \n",
    "    init_scale=10, \n",
    "    starter_xs=starter_xs,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(data[:, 0], data[:, 1], alpha=0.1, label=\"data\")\n",
    "plt.scatter(starting_states[:, 0], starting_states[:, 1], alpha=0.1, label=\"starting samples\")\n",
    "plt.scatter(final_states[:, 0], final_states[:, 1], alpha=0.1, label=\"final samples\")\n",
    "plt.xlim(-15, 15)\n",
    "plt.ylim(-15, 15)\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from celluloid import Camera\n",
    "from tqdm.autonotebook import tqdm \n",
    "\n",
    "fig = plt.figure()\n",
    "camera = Camera(fig)\n",
    "\n",
    "for timepoint in tqdm(chain_samples.swapaxes(0, 1)[::10]):\n",
    "    plt.scatter(*timepoint.T, color=\"blue\", alpha=0.1)\n",
    "    camera.snap()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animation = camera.animate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "\n",
    "# display_html(animation)\n",
    "animation.save(\"sampling2.mp4\", dpi=300, fps=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, it's powerful to just \"see\" what's happening amongst the chain samples!\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "499aa38474d161e044ebb3be9240784e1719d4331ad512ef6546dcd230708004"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('score-models')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
