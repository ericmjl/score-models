{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false \n",
    "#| output: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import os \n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "import jax.numpy as np  # import here so that any warnings about no GPU are not shown in website.\n",
    "np.arange(3)\n",
    "import shutup\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalizing to Higher Dimensions\n",
    "\n",
    "Thus far, we've explored score models in the context of 1D data.\n",
    "This is intentional!\n",
    "By working out the core ideas in a single dimension,\n",
    "we can more easily reason about what actually is happening --\n",
    "humans are, after all, very good at thinking in 1D.\n",
    "In effect, we eliminate the cognitive load that comes with thinking multi-dimensionally.\n",
    "Through this, the framework of how to think about \n",
    "how to use score models to generate data is quite clear.\n",
    "Our ingredients are:\n",
    "\n",
    "- Data,\n",
    "- A trainable model that can approximate the score of our data (implying that yes, we will train that model!), and\n",
    "- A procedure for noising up data and reversing that process to re-generate new data.\n",
    "\n",
    "Alas, however, the world of data that inhabits our world is rarely just 1D.\n",
    "More often than not, the data that we will encounter is going to be multi-dimensional.\n",
    "To exacerbate the matter, our data are also oftentimes discrete and not continuous,\n",
    "such as text, protein sequences, and more.\n",
    "Do the ideas explored in 1D generalize to multiple dimensions?[^1]\n",
    "In this notebook, I want to show how we can generalize from 1D to 2D.\n",
    "(With a bit of hand-waving,\n",
    "I'll claim at the end that this all works in n-dimensions too!)\n",
    "\n",
    "[^1]: Of course, yes -- this is a rhetorical question --\n",
    "and the more important point here is figuring out \n",
    "what we need to do to generalize beyond 1D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data: 2D Gaussians and Half Moons\n",
    "\n",
    "In this anchoring example, \n",
    "we will explore how to train a score model\n",
    "on both the half-moons dataset and a simple 2D Gaussian.\n",
    "For ease of presentation, the code (as executed here) will only use the half-moons dataset\n",
    "but one flag at the top of the cell below, `MOONS = True`,\n",
    "can be switched to `MOONS = False` to switch to the 2D Gaussian dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| fig-cap: Sample synthetic data that we will be working with.\n",
    "#| label: fig-synthetic-data\n",
    "#| code-fold: true\n",
    "\n",
    "import jax.numpy as np \n",
    "from jax import random \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.datasets import make_moons\n",
    "import seaborn as sns\n",
    "\n",
    "# CHANGE THIS FLAG TO FALSE TO RUN CODE WITH 2D MIXTURE GAUSSIANS.\n",
    "MOONS = True\n",
    "\n",
    "if MOONS:\n",
    "    X, y = make_moons(n_samples=1000, noise=0.1, random_state=99)\n",
    "    # Scale the moons dataset to be of the same scale as the Gaussian dataset.\n",
    "    X = X * 10\n",
    "\n",
    "else:\n",
    "    key = random.PRNGKey(55)\n",
    "    k1, k2 = random.split(key, 2)\n",
    "\n",
    "    loc1 = np.array([0., 0.])\n",
    "    cov1 = np.array([[1., 0.], [0., 1.]])\n",
    "    x1 = random.multivariate_normal(k1, loc1, cov1, shape=(1000,))\n",
    "\n",
    "    loc2 = np.array([5., 5.])\n",
    "    cov2 = cov1 \n",
    "    x2 = random.multivariate_normal(k2, loc2, cov2, shape=(1000,))\n",
    "\n",
    "    X = np.concatenate([x1, x2])\n",
    "\n",
    "plt.scatter(*X.T)\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.xlabel(\"Data Dimension 1\")\n",
    "plt.ylabel(\"Data Dimension 2\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add noise to data\n",
    "\n",
    "Next we noise up the data.\n",
    "Strictly speaking with a constant drift term,\n",
    "we need only parameterize our diffusion term using `t` (time)\n",
    "and don't really need to use `diffrax`'s SDE capabilities.\n",
    "We can noise up data by applying a draw\n",
    "from an isotropic Gaussian with covariance equal to the time elapsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| fig-cap: Synthetic data at different noise scales.\n",
    "#| label: fig-synthetic-data-with-noise\n",
    "#| code-fold: true\n",
    "\n",
    "from jax import vmap\n",
    "from functools import partial\n",
    "import seaborn as sns \n",
    "\n",
    "def noise_batch(key, X: np.ndarray, t: float) -> np.ndarray:\n",
    "    \"\"\"Noise up one batch of data.\n",
    "    \n",
    "    :param x: One batch of data.\n",
    "        Should be of shape (1, n_dims).\n",
    "    :param t: Time scale at which to noise up.\n",
    "    :returns: A NumPy array of noised up data.\n",
    "    \"\"\"\n",
    "    if t == 0.0:\n",
    "        return X\n",
    "    cov = np.eye(len(X)) * t\n",
    "    return X + random.multivariate_normal(key=key, mean=np.zeros(len(X)), cov=cov)\n",
    "\n",
    "\n",
    "def noise(key, X, t):\n",
    "    keys = random.split(key, num=len(X))\n",
    "    return vmap(partial(noise_batch, t=t))(keys, X)\n",
    "\n",
    "from jax import random \n",
    "\n",
    "fig, axes = plt.subplots(figsize=(8, 8), nrows=3, ncols=3, sharex=True, sharey=True)\n",
    "\n",
    "ts = np.linspace(0.001, 10, 9)\n",
    "key = random.PRNGKey(99)\n",
    "noise_level_keys = random.split(key, 9)\n",
    "noised_datas = []\n",
    "for t, ax, key in zip(ts, axes.flatten(), noise_level_keys):\n",
    "    noised_data = noise(key, X, t)\n",
    "    noised_datas.append(noised_data)\n",
    "    ax.scatter(noised_data[:, 0], noised_data[:, 1], alpha=0.1)\n",
    "    ax.set_title(f\"{t:.2f}\")\n",
    "noised_datas = np.stack(noised_datas)\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity-check, we should ensure that `noised_data`'s shape is `(time, batch, n_data_dims)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noised_datas.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed it is!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can set up a score model to be trained on each time point's noised-up data.\n",
    "Here, we are going to use a feed forward neural network.\n",
    "The neural network needs to accept `x` and `t`;\n",
    "as with the previous chapter, we will be using a single neural network\n",
    "that learns to map input data and time to the approximated score function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models.models.sde import SDEScoreModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are curious, you can see how the `SDEScoreModel` class is defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import getsource\n",
    "\n",
    "print(getsource(SDEScoreModel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key design choice here is that \n",
    "time `t` is made part of the MLP's input by concatenation with `x`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we need a sanity-check that the model's forward pass works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from jax import vmap \n",
    "\n",
    "model = SDEScoreModel(data_dims=2)\n",
    "t = 3.0\n",
    "\n",
    "X_noised = noise(key, X, t)\n",
    "out = vmap(partial(model, t=t))(X_noised)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the shape is correct, we can be confident in the forward pass of the model working correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need the score matching loss function;\n",
    "it is identical to the one we used in the previous chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models.losses import joint_sde_score_matching_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that the loss function works without error first.\n",
    "Once again, this is a good practice sanity check to perform\n",
    "before we "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SDEScoreModel(data_dims=2)\n",
    "joint_sde_score_matching_loss(model, noised_datas, ts=ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity-check again,\n",
    "let us make sure that we can take the gradient of the loss function as well.\n",
    "To do so, we will use Equinox's `filter_value_and_grad`,\n",
    "which is a fancy version of JAX's `value_and_grad`\n",
    "that ensures that we calculate `value_and_grad` only on array-like arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx \n",
    "\n",
    "dloss = eqx.filter_value_and_grad(joint_sde_score_matching_loss)\n",
    "value, grads = dloss(model, noised_datas, ts=ts)\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "Now that we've seen the gradient function perform without errors,\n",
    "let's train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "model = SDEScoreModel()\n",
    "\n",
    "optimizer = optax.chain(\n",
    "    optax.adam(5e-3),\n",
    "    optax.clip(0.01),\n",
    ")\n",
    "\n",
    "opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
    "dloss = eqx.filter_value_and_grad(joint_sde_score_matching_loss)\n",
    "\n",
    "n_steps = 5_000 # 13_000\n",
    "iterator = tqdm(range(n_steps))\n",
    "loss_history = []\n",
    "key = random.PRNGKey(555)\n",
    "keys = random.split(key, n_steps)\n",
    "\n",
    "updated_score_model = model\n",
    "for step in iterator:\n",
    "    loss_score, grads = dloss(updated_score_model, noised_datas, ts)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    updated_score_model = eqx.apply_updates(updated_score_model, updates)\n",
    "    iterator.set_description(f\"Score· {loss_score:.2f}\")\n",
    "    loss_history.append(float(loss_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the losses so we can have visual confirmation \n",
    "that we have trained the model to convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| fig-cap: Training loss curve.\n",
    "#| label: fig-training-loss·\n",
    "#| code-fold: true\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Score Matching Loss\")\n",
    "plt.title(\"Score Matching Loss History\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize gradient field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular case,\n",
    "because we have 2D data,\n",
    "one way of confirming that we have trained the model correctly\n",
    "is to look at the gradient field given by our trained score model.\n",
    "We will compare a trained model (on the left)\n",
    "to an untrained model (on the right).\n",
    "We should see that the gradient field points to the direction of highest data density,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| fig-cap: Gradient field of trained vs. untrained models at varying time points (corresponding to different noise scales).\n",
    "#| label: fig-gradient-field\n",
    "#| code-fold: true\n",
    "\n",
    "basic_size = 5\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    figsize=(2 * basic_size, len(noised_datas) * basic_size),\n",
    "    nrows=len(noised_datas), \n",
    "    ncols=2, \n",
    "    sharex=True, \n",
    "    sharey=True\n",
    ")\n",
    "init_model = SDEScoreModel()\n",
    "\n",
    "\n",
    "for idx in range(len(noised_datas)):\n",
    "\n",
    "    ax_row = axes[idx, :]\n",
    "    n_points = 20\n",
    "    xs = np.linspace(noised_datas[-1][:, 0].min(), noised_datas[-1][:, 0].max(), n_points)\n",
    "    ys = np.linspace(noised_datas[-1][:, 1].min(), noised_datas[-1][:, 1].max(), n_points)\n",
    "    xxs, yys = np.meshgrid(xs, ys)\n",
    "\n",
    "    x_y_pair = np.vstack([xxs.flatten(), yys.flatten()]).T\n",
    "    x_y_pair.shape\n",
    "\n",
    "    gradient_field = vmap(partial(updated_score_model, t=ts[idx]))(x_y_pair)\n",
    "\n",
    "    vect_length_scale = 1\n",
    "    vect_width = 0.1\n",
    "\n",
    "    for xy_pair, vect in zip(x_y_pair, gradient_field):\n",
    "        ax_row[0].arrow(*xy_pair, *vect * vect_length_scale, width=vect_width, alpha=0.1)    \n",
    "    ax_row[0].scatter(*noised_datas[idx].T, alpha=0.1, color=\"black\")\n",
    "    ax_row[0].set_xlim(noised_datas[idx][:, 0].min() - 1, noised_datas[idx][:, 0].max() + 1)\n",
    "    ax_row[0].set_ylim(noised_datas[idx][:, 1].min() - 1, noised_datas[idx][:, 1].max() + 1)\n",
    "    ax_row[0].set_title(f\"Trained Score Model at t={ts[idx]:.2f}\")\n",
    "    ax_row[0].set_xlabel(\"Data Dim 1\")\n",
    "    ax_row[0].set_ylabel(\"Data Dim 2\")\n",
    "\n",
    "\n",
    "    gradient_field = vmap(partial(init_model, t=ts[idx]))(x_y_pair)\n",
    "\n",
    "    for xy_pair, vect in zip(x_y_pair, gradient_field):\n",
    "        ax_row[1].arrow(*xy_pair, *vect * vect_length_scale, width=vect_width, alpha=0.1)    \n",
    "    ax_row[1].scatter(*noised_datas[idx].T, alpha=0.1, color=\"black\")\n",
    "    ax_row[1].set_xlim(noised_datas[idx][:, 0].min() - 1, noised_datas[idx][:, 0].max() + 1)\n",
    "    ax_row[1].set_ylim(noised_datas[idx][:, 1].min() - 1, noised_datas[idx][:, 1].max() + 1)\n",
    "    ax_row[1].set_title(f\"Untrained Score Model at t={ts[idx]:.2f}\")\n",
    "    ax_row[1].set_xlabel(\"Data Dim 1\")\n",
    "    ax_row[1].set_ylabel(\"Data Dim 2\")\n",
    "\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the gradient field on the right half of @fig-gradient-field\n",
    "consistently ignores the density of data,\n",
    "whereas the gradient field on the left half of @fig-gradient-field \n",
    "consistently points towards areas of high density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Flow ODE\n",
    "\n",
    "With the gradient fields confirmed to be correct,\n",
    "we can set up the probability flow ODE.\n",
    "\n",
    "We need a constant drift term, a time-dependent diffusion term,\n",
    "and finally, the updated score model inside there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constant_drift(t, y, args):\n",
    "    \"\"\"Constant drift term.\"\"\"\n",
    "    return 0\n",
    "\n",
    "def time_dependent_diffusion(t, y, args):\n",
    "    \"\"\"Diffusion term that increases with time.\"\"\"\n",
    "    return t * np.eye(2)\n",
    "\n",
    "\n",
    "def reverse_drift(t: float, y: float, args: tuple):\n",
    "    f = constant_drift(t, y, args)  # always 0, so we can, in principle, take this term out.\n",
    "    g = time_dependent_diffusion(t, y, args)\n",
    "    s = updated_score_model(y, t)\n",
    "    # Extract out the diagonal because we assume isotropic Gaussian noise is applied.\n",
    "    return f - 0.5 * np.diagonal(np.linalg.matrix_power(g, 2)) * s\n",
    "\n",
    "from diffrax import ODETerm, Tsit5, SaveAt, diffeqsolve\n",
    "\n",
    "class ODE(eqx.Module):\n",
    "    drift: callable\n",
    "\n",
    "    def __call__(self, ts: np.ndarray, y0: float):\n",
    "        term = ODETerm(self.drift)\n",
    "        solver = Tsit5()\n",
    "        saveat = SaveAt(ts=ts, dense=True)\n",
    "        sol = diffeqsolve(\n",
    "            term, solver, t0=ts[0], t1=ts[-1], dt0=ts[1] - ts[0], y0=y0, saveat=saveat\n",
    "        )\n",
    "        return vmap(sol.evaluate)(ts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the probability flow trajectories from a random sampling of starter points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true \n",
    "#| output: false\n",
    "\n",
    "ode = ODE(reverse_drift)\n",
    "ts = np.linspace(9, 0.0001, 43)\n",
    "key = random.PRNGKey(39)\n",
    "y0s = random.multivariate_normal(key, mean=np.array([0.0, 0.0]), cov=np.eye(2), shape=(50,)) * 30 + np.ones(2) * 5\n",
    "\n",
    "trajectories = []\n",
    "for y0 in y0s:\n",
    "    trajectory = ode(ts, y0)\n",
    "    trajectories.append(trajectory)\n",
    "trajectories = np.stack(trajectories)\n",
    "\n",
    "from celluloid import Camera\n",
    "\n",
    "fig, axes = plt.subplots()\n",
    "\n",
    "camera = Camera(fig)\n",
    "# Plot the noised datas as a background\n",
    "plt.scatter(*noised_datas[0].T, alpha=0.05, color=\"black\")\n",
    "\n",
    "\n",
    "for idx in range(len(ts)):\n",
    "    plt.scatter(*noised_datas[0].T, alpha=0.05, color=\"black\")\n",
    "    plt.scatter(*trajectories[:, idx, :].T, marker=\"o\", color=\"blue\")\n",
    "    plt.gca().set_aspect(\"equal\")\n",
    "    plt.xlabel(\"Data Dim 1\")\n",
    "    plt.ylabel(\"Data Dim 2\")\n",
    "    sns.despine()\n",
    "    camera.snap()\n",
    "\n",
    "animation = camera.animate()\n",
    "animation.save(\"probability-flow-ode.mp4\", writer=\"ffmpeg\", dpi=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| fig-cap: Probability flow ODE and trajectories from a variety of randomly-chosen starting points. Circles mark the starting location, while diamonds mark the ending location of each trajectory.\n",
    "#| label: fig-prob-flow-ode\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(animation.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in @fig-prob-flow-ode,\n",
    "with an (admittedly not so) random selection of starter points,\n",
    "we can run the probability flow ODE in reverse time to get data coordinates \n",
    "that are distributed like our original starter data...\n",
    "without knowing the original data generating distribution!\n",
    "This is the whole spirit of score-based models,\n",
    "and in this chapter, we explored how to make that happen in a non-trivial 2D case.\n",
    "In principle, we could run with any kind of numerical data,\n",
    "such as images (where the original application of score models was done),\n",
    "or numerically embedded text (or protein sequences)\n",
    "from an encoder-decoder pair's encoder module."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "499aa38474d161e044ebb3be9240784e1719d4331ad512ef6546dcd230708004"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
