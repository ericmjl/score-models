{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to need some basic knowledge and terminology established first,\n",
    "otherwise, the terminology may become overwhelming,\n",
    "especially for those who are not well-versed in probabilistic modelling.\n",
    "As such, we're going to start with a bunch of definitions.\n",
    "Don't skip these, they're important!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's a score function?\n",
    "The score function is defined as follows:\n",
    "\n",
    "> The score function is\n",
    "> the gradient of the log of the probability density function \n",
    "> of a probability distribution\n",
    "> with respect to the distribution's support.\n",
    "\n",
    "There's a lot to unpack in there, \n",
    "so let's dissect the anatomy of this definition bit by bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Distributions\n",
    "\n",
    "Probability distributions are super cool objects in stats[^bayes].\n",
    "Distributions can be **configured** through their parameters;\n",
    "for example, by setting the values $\\mu$ and $\\sigma$ of a Gaussian respectively.\n",
    "We can use probability distributions to generate data, \n",
    "and we can use them to evaluate the likelihood of observed data.\n",
    "The latter point is done by using a probability distribution's\n",
    "**probability density function**[^discrete].\n",
    "\n",
    "[^bayes]: I've explored the anatomy of a probability distribution\n",
    "in my essay on [Bayesian and computational statistics][bayes],\n",
    "and would recommend looking at it for a refresher.\n",
    "\n",
    "[^discrete]: Or the probability mass function, for discrete distributions,\n",
    "but we're going to stick with continuous distributions for this essay.\n",
    "\n",
    "[bayes]: https://ericmjl.github.io/essays-on-data-science/machine-learning/computational-bayesian-stats/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $P(x)$ a.k.a. Probability Density Function\n",
    "\n",
    "A distribution's probability density function (PDF)\n",
    "describes the propensity of a probability distribution\n",
    "to generate draws of a particular value.\n",
    "As mentioned above, we primarily use the PDF to\n",
    "_evaluate the likelihood of the observing data, given the distribution's configuration_.\n",
    "If you need an anchoring example, \n",
    "think of the venerable Gaussian probability density function in @fig-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false \n",
    "#| output: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import os \n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "import jax.numpy as np  # import here so that any warnings about no GPU are not shown in website.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "#| label: fig-likelihood\n",
    "#| fig-cap: \"$P(x)$ (likelihood, PDF), $log P(x)$ (log likelihood, logp), and $dlogP(x)$ (score) of a Gaussian.\"\n",
    "#| code-fold: true\n",
    "\n",
    "from jax.scipy.stats import norm\n",
    "import jax.numpy as np\n",
    "from jax import grad, vmap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    " \n",
    "fig, axes = plt.subplots(figsize=(8, 3), nrows=1, ncols=3, sharex=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "x = np.linspace(-3.5, 3.5, 1000)\n",
    "y = norm.pdf(x)\n",
    "plt.plot(x, y, color=\"black\")\n",
    "plt.xlabel(\"Support\")\n",
    "plt.ylabel(\"Likelihood\")\n",
    "plt.title(\"PDF\")\n",
    "sns.despine()\n",
    "\n",
    "y = norm.logpdf(x)\n",
    "plt.sca(axes[1])\n",
    "plt.plot(x, y, color=\"black\")\n",
    "plt.xlabel(\"Support\")\n",
    "plt.ylabel(\"logP(x)\")\n",
    "plt.title(\"Log PDF\")\n",
    "\n",
    "# Tangent Line\n",
    "def line(x):\n",
    "    return grad(norm.logpdf)(x_pt) * (x - x_pt) + y_pt\n",
    "x_pt = -1.5\n",
    "y_pt = norm.logpdf(x_pt)\n",
    "xrange = np.linspace(x_pt - 1, x_pt + 1, 10)\n",
    "plt.plot(x, y, color=\"black\")\n",
    "plt.scatter(x_pt, y_pt, color=\"gray\")\n",
    "plt.plot(xrange, vmap(line)(xrange), color=\"gray\", ls=\"--\")\n",
    "\n",
    "\n",
    "plt.sca(axes[2])\n",
    "plt.plot(x, vmap(grad(norm.logpdf))(x), color=\"black\")\n",
    "plt.axhline(y=0, ls=\"--\", color=\"black\")\n",
    "plt.axvline(x=0, ls=\"--\", color=\"black\")\n",
    "plt.xlabel(\"Support\")\n",
    "plt.ylabel(\"dlogP(x)\")\n",
    "plt.title(\"Score\")\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every distribution has a **support**,\n",
    "which is the range of values for which the probability distribution is defined.\n",
    "The Gaussian has support in the range $(-\\infty, \\infty)$,\n",
    "while positive-only distributions (such as the Exponential)\n",
    "have support in the range $(0, \\infty)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $log P(x)$ a.k.a. Log PDF\n",
    "\n",
    "Because the PDF is nothing more than a math function, we can take its logarithm!\n",
    "In computational statistics, taking the log is usually done for pragmatic purposes,\n",
    "as we usually end up with underflow issues otherwise.\n",
    "For the standard Gaussian above, its log PDF looks like what we see in @fig-likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often call the log PDF **logp** for short,\n",
    "and in the probabilistic programming language PyMC,\n",
    "`logp` is the name of the class method use for calculating \n",
    "the log likelihood of data under the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $dlogP(x)$ a.k.a. Score Function\n",
    "\n",
    "Finally, we get to the **score**.\n",
    "As it turns out, because the logp function is differentiable,\n",
    "we can take its derivative easily (using JAX, for example).\n",
    "The derivative of the logp function is called the **score function**.\n",
    "The score of a distribution is the gradient of the logp function w.r.t. the support.\n",
    "You can visualize what it is like in @fig-likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In JAX, obtaining the score function is relatively easy.\n",
    "We simply need to use JAX's `grad` function to obtain the transformed logp function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad \n",
    "gaussian_score = grad(norm.logpdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're using a Gaussian as our anchoring example,\n",
    "let's examine some properties of the score function.\n",
    "From visual inspection above,\n",
    "we know that at the top of the Gaussian,\n",
    "the gradient should be zero,\n",
    "and can verify as much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_score(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the tails, the gradient should be of higher magnitude\n",
    "than at the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_score(-3.0), gaussian_score(3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the score function\n",
    "\n",
    "In 2005, Aapo Hyvärinen published a paper in the Journal of Machine Learning Research\n",
    "that details how to _estimate_ the score function\n",
    "in the absence of knowledge of \n",
    "the true data generating distribution [@JMLR:v6:hyvarinen05a].\n",
    "When I first heard of the idea, I thought it was crazy --\n",
    "crazy cool that we could even do this!\n",
    "\n",
    "[hyvarinen]: https://jmlr.csail.mit.edu/papers/volume6/hyvarinen05a/old.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we use data to estimate the score of that data \n",
    "without knowing the true probability density?\n",
    "One key equation in the paper is equation #4.\n",
    "This equation details how we can use an arbitrary function, $\\psi(x, \\theta)$,\n",
    "to approximate the score function,\n",
    "and the loss function needed to train the parameters of the function $\\theta$\n",
    "to approximate the score function.\n",
    "I've replicated the equation below,\n",
    "alongside a bullet-point explanation of what each of the terms are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{i=1}^{n} [\\delta_i \\psi_i(x(t); \\theta) + \\frac{1}{2} \\psi_i(x(t); \\theta)^2 ] + \\text{const}$$\n",
    "\n",
    "Here:\n",
    "\n",
    "- $J(\\theta)$ is the loss function that we wish to minimize w.r.t. the parameters $\\theta$\n",
    "- $\\theta$ are the parameters of the function $\\psi_i$\n",
    "- $\\psi_i(x(t); \\theta)$ is the multidimensional score function estimator for $x$. $\\psi_i$ has parameters $\\theta$. \n",
    "  - The subscript $i$ is a dimensional indexer. If $x$ is 2-dimensional, then $i=2$.\n",
    "- $x(t)$ are the i.i.d. samples from the unknown data-generating distribution.\n",
    "- $\\delta_i$ refers to the partial derivative w.r.t. dimension $i$ in $x$.\n",
    "- $\\text{const}$ is a constant term that effectively can be ignored.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the idea in a bit more detail.\n",
    "What we're ultimately going to do here \n",
    "is use a simple feed-forward neural network\n",
    "as the score function estimator $\\psi(x(t), \\theta)$.\n",
    "Let's start first by generating the kind of data that's needed\n",
    "for score function estimation to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "from jax import random\n",
    "\n",
    "key = random.PRNGKey(44)\n",
    "\n",
    "true_mu = 3.0\n",
    "true_sigma = 1.0\n",
    "data = random.normal(key, shape=(1000, 1)) * true_sigma + true_mu\n",
    "data[0:5]  # showing just the first 10 samples drawn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go on, we should also verify that the $\\mu$ and $\\sigma$ of the data \n",
    "are as close to the ground truth as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.mean(), data.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: evaluate score under known PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to evaluate the score function directly.\n",
    "The purpose here is to establish a baseline model to compare against\n",
    "and to set up the patterns for training a neural network model.\n",
    "In anticipation of writing neural network models later,\n",
    "I have opted to write our models, neural network or otherwise,\n",
    "in the style of Equinox [@kidger2021equinox],\n",
    "which provides a way to associate model parameters with a callable object directly\n",
    "while maintaining compatibility with the rest of the JAX ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models.models.gaussian import GaussianModel\n",
    "from inspect import getsource\n",
    "\n",
    "print(getsource(GaussianModel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, instead of `grad`, we are using `jacfwd`,\n",
    "which gives us the Jacobian of `norm.logpdf`.\n",
    "The Jacobian is a generalization of the first derivative, extended to matrix inputs.\n",
    "To test that we have the implementation done correct,\n",
    "let's ensure that we can evaluate the `GaussianModel` score function\n",
    "at a few special points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_model = GaussianModel()\n",
    "(\n",
    "    gaussian_model(-3.0),\n",
    "    gaussian_model(0.0),\n",
    "    gaussian_model(3.0),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also ensure that we can evalute the score function at each data point.\n",
    "We will use the `vmap` function to explicitly map `score_func` across all data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_score = vmap(gaussian_model)(data).squeeze()\n",
    "data_score[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train baseline score function model\n",
    "\n",
    "Here, we've instantiated the Gaussian with default parameters ($\\mu=0$ and $\\sigma=1$),\n",
    "but those aren't the true configuration of the underlying data-generating process.\n",
    "Hence, our score calculated scores are going to be way off,\n",
    "as is visible in @fig-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| echo: false\n",
    "#| label: fig-score\n",
    "#| fig-cap: \"Comparison of score function evaluated under the true vs. incorrect data-generating distribution parameters\"\n",
    "\n",
    "import numpy as onp \n",
    "\n",
    "true_model = GaussianModel(mu=true_mu, log_sigma=np.log(true_sigma))\n",
    "true_model_scores = vmap(true_model)(data)\n",
    "plt.scatter(data.squeeze(), true_model_scores.squeeze(), label=\"True Score Model\")\n",
    "plt.scatter(data.squeeze(), data_score.squeeze(), label=\"Initialized Score Model\")\n",
    "plt.xlabel(\"Support\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model is wrong, we're going to see if we can make it right.\n",
    "One generic way to train models is to use gradient descent;\n",
    "that's what we'll use here.\n",
    "For us, we'll be using optax \n",
    "alongside a fitting routine that I have implemented before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we've reached the point where we can implement the score function loss in JAX!\n",
    "Let's see it below, with the earlier equation from above copied down here for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{i=1}^{n} [\\delta_i \\psi_i(x(t); \\theta) + \\frac{1}{2} \\psi_i(x(t); \\theta)^2 ] + \\text{const}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models.training import fit\n",
    "from score_models.losses import score_matching_loss\n",
    "import optax\n",
    "\n",
    "optimizer = optax.adam(learning_rate=5e-3)\n",
    "updated_model, loss_history = fit(\n",
    "    gaussian_model, \n",
    "    data, \n",
    "    score_matching_loss, \n",
    "    optimizer, \n",
    "    steps=2_000, \n",
    "    progress_bar=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at the loss curve to make sure our model is fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| label: fig-gaussian-model-loss-history\n",
    "#| fig-cap: \"Loss curve for Gaussian score model.\"\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(8, 4), ncols=2)\n",
    "\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Training Iteration\")\n",
    "plt.ylabel(\"Score Matching Loss\")\n",
    "plt.title(\"Score Matching Loss History\")\n",
    "sns.despine()\n",
    "\n",
    "plt.sca(axes[1])\n",
    "updated_model_scores = vmap(updated_model)(data)\n",
    "plt.scatter(updated_model_scores.squeeze(), true_model_scores.squeeze())\n",
    "plt.xlabel(\"Estimated Scores\")\n",
    "plt.ylabel(\"True Model\")\n",
    "plt.title(\"True vs. Estimated Score\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's reassuring to see the loss decrease and the estimated scores match up with the predicted scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are the mean and standard deviation of the data vs. the model estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(data), np.std(data), updated_model.mu, np.exp(updated_model.log_sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate Score Functions\n",
    "\n",
    "Now, the entire premise of Hyvärinen's paper is that \n",
    "we need not know the original form of the PDF\n",
    "and we'd still be able to _estimate_ the score at each data point.\n",
    "Since the score function is smooth and differentiable,\n",
    "we can reach for the venerable neural network,\n",
    "a.k.a. the universal function approximator,\n",
    "as our estimation model for the score of our data.\n",
    "\n",
    "### Neural Network Approximator\n",
    "\n",
    "Here, we will set up a single feed-forward neural network model\n",
    "with 1 hidden layer of width 1024 and a ReLU activation function.\n",
    "Here is the source of my wrapper implementation around Equinox's MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models.models.feedforward import FeedForwardModel1D\n",
    "\n",
    "print(getsource(FeedForwardModel1D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Objective/Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our loss function, I know from previous experience with these models\n",
    "that we could get weights that explode to really large magnitudes.\n",
    "To control this, \n",
    "I have chained in an L2 regularization on the weights on the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import nn \n",
    "from score_models import losses\n",
    "\n",
    "regularized_loss = losses.chain(\n",
    "    losses.l2_norm, \n",
    "    losses.score_matching_loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffmodel = FeedForwardModel1D(depth=1, width_size=1024, activation=nn.relu)\n",
    "optimizer = optax.chain(\n",
    "    optax.clip(0.01),\n",
    "    optax.sgd(learning_rate=5e-3),\n",
    ")\n",
    "updated_model, history = fit(\n",
    "    ffmodel,\n",
    "    data,\n",
    "    regularized_loss,\n",
    "    optimizer,\n",
    "    steps=2_000,\n",
    "    progress_bar=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximator Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize the training loss and how the model compares to ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: fig-nn-model-loss-and-perf\n",
    "#| fig-cap: \"Loss curve and performance plot for Neural Network score model.\"\n",
    "#| code-fold: true\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(8, 4), ncols=2)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Training Iteration\")\n",
    "plt.ylabel(\"Loss Value\")\n",
    "plt.title(\"Score Matching Loss History\")\n",
    "sns.despine()\n",
    "\n",
    "plt.sca(axes[1])\n",
    "updated_model_scores = vmap(updated_model)(data).squeeze()\n",
    "plt.scatter(data.squeeze(), true_model_scores, label=\"True Model Scores\")\n",
    "plt.scatter(data.squeeze(), updated_model_scores, label=\"Feed Forward Estimate\")\n",
    "plt.xlabel(\"Support\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"True vs. Estimated Score\")\n",
    "plt.legend()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't bad at all!\n",
    "We're off by a bit, \n",
    "but keep in mind that we only had data on hand\n",
    "and didn't know what the exact data-generating density is.\n",
    "We should expect to be a bit off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture Distributions\n",
    "\n",
    "While data drawn from a Gaussian is nice and ideal,\n",
    "you won't really be able to tell if your data came from a Gaussian[^downey].\n",
    "In addition, most data would have the characteristics\n",
    "of being generated from a mixture distribution.\n",
    "In other words, mixture distributions are what our data will look the most like.\n",
    "Let's make sure our approximate score function \n",
    "can approximate the mixture distribution scores as accurately as possible,\n",
    "at least in 1 dimension.\n",
    "\n",
    "[^downey]: Allen Downey has an [excellent blog post][downey] on this matter.\n",
    "\n",
    "[downey]: http://allendowney.blogspot.com/2013/08/are-my-data-normal.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixture Gaussian Model\n",
    "\n",
    "We have a `MixtureGaussian` model \n",
    "that implements the score of a mixture Gaussian distribution.\n",
    "Its source is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models.models.gaussian import MixtureGaussian\n",
    "\n",
    "print(getsource(MixtureGaussian))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixture Gaussian Score Function \n",
    "\n",
    "Let's use the model to plot Gaussian data and the true Gaussian mixture score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: fig-two-comp-mixture-gaussian\n",
    "#| fig-cap: A two-component mixture Gaussian and its score function\n",
    "#| code-fold: true\n",
    "\n",
    "import seaborn as sns \n",
    "import numpy as onp \n",
    "\n",
    "x = np.linspace(-10, 10, 200)\n",
    "mus = np.array([-3, 3])\n",
    "sigmas = np.array([1, 1])\n",
    "ws = np.array([0.1, 0.9])\n",
    "\n",
    "mgmodel = MixtureGaussian(mus, np.log(sigmas), ws)\n",
    "mixture_logpdf_grads = vmap(mgmodel)(x)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(8, 4), ncols=2)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "k1, k2 = random.split(random.PRNGKey(55))\n",
    "draws = 1000\n",
    "mix1 = random.normal(k1, shape=(1000,)) * 1 - 3\n",
    "mix2 = random.normal(k2, shape=(9000,)) * 1 + 3\n",
    "data = np.concatenate([mix1, mix2]).reshape(-1, 1)\n",
    "plt.hist(onp.array(data), bins=100)\n",
    "plt.title(\"Mixture Gaussian Histogram\")\n",
    "plt.xlabel(\"Support\")\n",
    "plt.ylabel(\"Count\")\n",
    "sns.despine()\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plt.plot(x, mixture_logpdf_grads)\n",
    "plt.xlabel(\"Support\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Mixture Gaussian Score\")\n",
    "sns.despine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train neural network approximator\n",
    "\n",
    "Then, we're going to use the feed forward neural network model from before\n",
    "to try to fit the mixture Gaussian data above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.chain(\n",
    "    optax.clip(0.01),\n",
    "    optax.sgd(learning_rate=5e-3),\n",
    ")\n",
    "updated_model, history = fit(\n",
    "    ffmodel,\n",
    "    data,\n",
    "    losses.score_matching_loss,\n",
    "    optimizer,\n",
    "    steps=1_000,\n",
    "    progress_bar=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the loss looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: fig-nn-model-mixture-loss-history\n",
    "#| fig-cap: \"Loss curve for neural network score model approximator for 2-component mixture Gaussian.\"\n",
    "#| code-fold: true\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(8, 4), ncols=2)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Training Iteration\")\n",
    "plt.ylabel(\"Score Matching Loss\")\n",
    "plt.title(\"Score Matching Loss History\")\n",
    "sns.despine()\n",
    "\n",
    "plt.sca(axes[1])\n",
    "mixture_est_scores = vmap(updated_model)(data)\n",
    "plt.plot(x, mixture_logpdf_grads, label=\"Ground Truth\")\n",
    "plt.scatter(data, mixture_est_scores, label=\"Estimated\")\n",
    "plt.xlabel(\"Support\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"True vs. Estimated Score\")\n",
    "plt.legend()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! \n",
    "It's clear to me that we can approxiate \n",
    "the score function of a 2-component mixture Gaussian here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-Component Mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now going to see whether we can approximate a 3-component mixture.\n",
    "This will be the example that rounds out this chapter.\n",
    "Firstly, let's see draws from a 3-component mixture model\n",
    "and the score function of this mixture distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: fig-3-comp-gaussian-mixture-score-func\n",
    "#| fig-cap: 3-component mixture Gaussian score function.\n",
    "#| code-fold: true\n",
    "\n",
    "mus = np.array([-7, -2, 3])\n",
    "sigmas = np.ones(3)\n",
    "ws = np.array([0.1, 0.4, 0.5])\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(8, 4), ncols=2, sharex=True)\n",
    "plt.sca(axes[0])\n",
    "k1, k2, k3 = random.split(random.PRNGKey(91), 3)\n",
    "draws = 1000\n",
    "mix1 = random.normal(k1, shape=(100,)) * 1 - 7\n",
    "mix2 = random.normal(k2, shape=(400,)) * 1 - 2\n",
    "mix3 = random.normal(k3, shape=(500,)) * 1 + 3\n",
    "data = np.concatenate([mix1, mix2, mix3]).reshape(-1, 1)\n",
    "plt.hist(onp.array(data), bins=100)\n",
    "plt.xlabel(\"Support\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Mixture Gaussian Histogram\")\n",
    "sns.despine()\n",
    "\n",
    "plt.sca(axes[1])\n",
    "x = np.linspace(-11, 6, 1000)\n",
    "three_comp_gaussian = MixtureGaussian(mus, np.log(sigmas), ws)\n",
    "\n",
    "mixture_logpdf_grads = vmap(three_comp_gaussian)(x)\n",
    "plt.plot(x, mixture_logpdf_grads)\n",
    "plt.xlabel(\"Support\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Mixture Gaussian Score\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train a 3-component Gaussian model and check its performance.\n",
    "This again serves as our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check that this works with a MixtureGaussianModel\n",
    "three_comp_gaussian_est = MixtureGaussian(mus, np.log(sigmas), np.ones(3) / 3)\n",
    "optimizer = optax.chain(\n",
    "    optax.clip(0.001),\n",
    "    optax.adam(learning_rate=5e-3),\n",
    ")\n",
    "updated_model, history = fit(\n",
    "    three_comp_gaussian_est, data, losses.score_matching_loss, optimizer, steps=100, progress_bar=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: fig-ground-truth-3-comp-gaussian\n",
    "#| fig-cap: Training performance of a 3-component Gaussian model trained on the sampled data.\n",
    "#| code-fold: true\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(8, 4))\n",
    "plt.sca(axes[0])\n",
    "plt.plot(history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Score Matching Loss History\")\n",
    "sns.despine()\n",
    "\n",
    "\n",
    "plt.sca(axes[1])\n",
    "mixture_est_scores = vmap(updated_model)(data)\n",
    "plt.plot(x, mixture_logpdf_grads, label=\"Ground Truth\")\n",
    "plt.scatter(data, mixture_est_scores, label=\"Estimated Scores\")\n",
    "plt.xlabel(\"Support\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"True vs. Baseline Score\")\n",
    "sns.despine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're going to train a neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "# NOTE: I needed to tweak this neural network's\n",
    "# activation function, architecture, and number of training steps quite a bit.\n",
    "# Recording here the thing that trains most stably:\n",
    "# - 2000 steps\n",
    "# - depth = 2, width_size = 512, activation = nn.softplus\n",
    "# - optax.clip(0.001), optax.adam(learning_rate = 5e-3)\n",
    "# - lossses.score_matching_loss, don't do regularized.\n",
    "\n",
    "ffmodel = FeedForwardModel1D(depth=2, width_size=512, activation=nn.softplus)\n",
    "optimizer = optax.chain(\n",
    "    optax.clip(0.0001),\n",
    "    optax.adam(learning_rate=5e-3),\n",
    ")\n",
    "\n",
    "updated_model, history = fit(\n",
    "    ffmodel,\n",
    "    data,\n",
    "    losses.score_matching_loss,\n",
    "    optimizer,\n",
    "    steps=3_000,\n",
    "    progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| fig-cap: Training performance of feed forward neural network on 3-component Gaussian mixture.\n",
    "#| label: fig-3comp-mixture-nn-performance\n",
    "#| code-fold: true\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(8, 4))\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plt.plot(history)\n",
    "plt.title(\"Score Matching Loss History\")\n",
    "sns.despine()\n",
    "\n",
    "plt.sca(axes[1])\n",
    "mixture_est_scores = vmap(updated_model)(data)\n",
    "plt.plot(x, mixture_logpdf_grads, label=\"True\")\n",
    "plt.scatter(data, mixture_est_scores, label=\"Estimated\")\n",
    "plt.title(\"True vs. Estimated Score\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, this actually works!\n",
    "Although in this case we do know the true score function,\n",
    "we are actually trying to _estimate_ it \n",
    "in the presence of draws from the data-generating distribution\n",
    "while pretending to not know the true data-generating distribution.\n",
    "What I've tried to show here is that \n",
    "a neural network model can approximate the true score function,\n",
    "as shown in @fig-3comp-mixture-nn-performance and @fig-nn-model-mixture-loss-history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up Next\n",
    "\n",
    "Coming up next is how we _sample_ from a distribution\n",
    "when knowing _only_ the score function, true or estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96a062a7e1adbb829192b5a56a463e7bc3d2201a3b05feadd05a7a64f9805fbb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
