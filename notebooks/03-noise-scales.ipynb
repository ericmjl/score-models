{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false \n",
    "#| output: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import os \n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "import jax.numpy as np  # import here so that any warnings about no GPU are not shown in website.\n",
    "np.arange(3)\n",
    "import shutup\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise Scales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem with naÃ¯ve score approximation\n",
    "\n",
    "With a score function approximator, we have one small issue:\n",
    "in regions of low sample density,\n",
    "our estimate of the score function will be inaccurate,\n",
    "simply because we have few samples in those regimes.\n",
    "To get around this, we can:\n",
    "\n",
    "> perturb data points with noise \n",
    "> and train score-based models on the noisy data points instead.\n",
    "> When the noise magnitude is sufficiently large, \n",
    "> it can populate low data density regions \n",
    "> to improve the accuracy of estimated scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a huge tradeoff here, though:\n",
    "the larger the amount of perturbation,\n",
    "the greater the corruption of the input data.\n",
    "Let's see that in action.\n",
    "\n",
    "As always, we'll start by generating some mixture Gaussian data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| fig-cap: Histogram and ECDF of two-component mixture Gaussian data. \n",
    "#| label: fig-data-hist-ecdf\n",
    "from jax import random, vmap\n",
    "import jax.numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as onp \n",
    "import seaborn as sns \n",
    "\n",
    "def ecdf(data):\n",
    "    x, y = np.sort(data), np.arange(1, len(data) + 1) / len(data)\n",
    "    return x, y\n",
    "\n",
    "key = random.PRNGKey(45)\n",
    "k1, k2, k3 = random.split(key, 3)\n",
    "\n",
    "locations = [-15, 0, 15]\n",
    "\n",
    "mix1 = random.normal(k1, shape=(1000,)) * 1 + locations[0]\n",
    "mix2 = random.normal(k2, shape=(500,)) * 1 + locations[1]\n",
    "mix3 = random.normal(k2, shape=(500,)) * 1 + locations[2]\n",
    "\n",
    "data = np.concatenate([mix1, mix2, mix3]).reshape(-1, 1)\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(8, 4), ncols=2, sharex=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plt.hist(onp.array(data), bins=100)\n",
    "plt.xlabel(\"Support\")\n",
    "plt.title(\"Histogram\")\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plt.plot(*ecdf(data.flatten()))\n",
    "plt.xlabel(\"Support\")\n",
    "plt.ylabel(\"Cumulative Fraction\")\n",
    "plt.title(\"ECDF\")\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we have intentionally spaced out the Gaussians\n",
    "to create a region of extremely low density (in the approximate region $(-5, 4)$).\n",
    "As we'll see later, in this region, the gradients will be really hard to estimate,\n",
    "and the errors in this region may be pretty large.\n",
    "\n",
    "Next up, we're going to perturb that data.\n",
    "What we do here is add standard Gaussian draws to each data point over 5 noising steps.\n",
    "Progressively, the draws should converge on a very smooth Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial \n",
    "noise_scale = np.linspace(1, 8, 9)\n",
    "\n",
    "def noise(data, scale, key):\n",
    "    draw = random.normal(key, shape=data.shape) * scale \n",
    "    return data + draw \n",
    "\n",
    "keys = random.split(k3, len(noise_scale))\n",
    "\n",
    "data_perturbed = vmap(partial(noise, data))(noise_scale, keys)\n",
    "data_perturbed.shape  # (num_noise_scales, num_samples, data_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| fig-cap: Distribution of samples with and without perturbation. (Top three rows) Histograms of the data. (Bottom) Empirical cumulative distribution function of samples with differing levels of perturbation.\n",
    "#| label: fig-noised-data-dist\n",
    "#| code-fold: true\n",
    "fig, axes = plt.subplot_mosaic(\"\"\"\n",
    "ABC\n",
    "DEF\n",
    "GHI\n",
    "JJJ\n",
    "\"\"\",\n",
    "    figsize=(8, 10.5), sharex=True)\n",
    "\n",
    "ax_keys = \"ABCDEFGHI\"\n",
    "for i, (row, scale, ax_key) in enumerate(zip(data_perturbed, noise_scale, ax_keys)):\n",
    "    plt.sca(axes[ax_key])\n",
    "    plt.hist(onp.array(row.flatten()), bins=100)\n",
    "    plt.title(f\"$\\sigma$={scale}\")\n",
    "    plt.xlabel(\"Support\")\n",
    "    for loc in locations: \n",
    "        plt.axvline(loc, color=\"black\", ls=\"--\")\n",
    "\n",
    "plt.sca(axes[\"J\"])\n",
    "for row, scale in zip(data_perturbed, noise_scale):\n",
    "    plt.plot(*ecdf(row.flatten()), label=f\"$\\sigma$={scale}\")\n",
    "    for loc in locations: \n",
    "        plt.axvline(loc, color=\"black\", ls=\"--\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Support\")\n",
    "plt.ylabel(\"Cumulative Fraction\")\n",
    "plt.title(\"ECDF\")\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should be evident from the figure above\n",
    "that when we add more noise, the data look more and more like a single Gaussian\n",
    "and less like the original.\n",
    "Most crucially, in the regions of low density between the two mixture Gaussians\n",
    "(the flat regime in the blue line),\n",
    "we have a region of high density in the perturbed distributions\n",
    "(the red line in the same region).\n",
    "We should be able to obtain accurate score models \n",
    "for the perturbed data in the regime of low density support (on the blue curve).\n",
    "As we will see later, \n",
    "this will help us obtain slightly more accurate score models\n",
    "for the blue curve's flat region.\n",
    "In theory, if we were to reverse the process,\n",
    "we should get back our original data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling using the score function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! With this, we are now ready to start sampling!\n",
    "This is the logic that we're going to follow.\n",
    "We know that the last perturbation's score models \n",
    "are going to be more accurate on the perturbed distribution,\n",
    "but it's also going to be less accurate about the original distribution.\n",
    "To recap, we're going to need a score model \n",
    "that approximates the score function of our data\n",
    "and a Langevin dynamics sampler for generating new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Score Model\n",
    "\n",
    "As always, we train the score model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models.training import fit\n",
    "from score_models.models import FeedForwardModel1D\n",
    "from score_models.losses import score_matching_loss\n",
    "from score_models.sampler import LangevinDynamicsChain\n",
    "from jax import nn\n",
    "import optax\n",
    "from functools import partial\n",
    "\n",
    "ffmodel = FeedForwardModel1D(depth=2, width_size=512, activation=nn.softplus)\n",
    "optimizer = optax.chain(\n",
    "    optax.adam(learning_rate=5e-3),\n",
    ")\n",
    "\n",
    "updated_model, history = fit(\n",
    "    ffmodel, data, score_matching_loss, optimizer, 2_000, progress_bar=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample\n",
    "\n",
    "Then, we sample new data using the score function coupled with a Langevin dynamics sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_particles = 20_000\n",
    "starter_points = random.normal(key, shape=(n_particles, 1, 1)) * 10\n",
    "starter_keys = random.split(key, n_particles)\n",
    "trained_model_sampler = LangevinDynamicsChain(\n",
    "    gradient_func=updated_model, n_samples=100, epsilon=5e-1\n",
    ")\n",
    "final_non_joint, trained_samples = vmap(trained_model_sampler)(starter_points, starter_keys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| fig-cap: Final samples from 10,000 particles run for 100 sampling steps, shown as (a) a histogram, and (b) an ECDF alongside the original data.\n",
    "#| label: fig-samples-10kparticles-100steps\n",
    "fig, axes = plt.subplots(figsize=(8, 3), ncols=3, nrows=1, sharex=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plt.plot(*ecdf(final_non_joint.flatten()), label=\"Sampled\")\n",
    "plt.plot(*ecdf(data.flatten()), label=\"Original\")\n",
    "for loc in locations: \n",
    "    plt.axvline(loc, color=\"black\", ls=\"--\")\n",
    "\n",
    "plt.xlabel(\"Support\")\n",
    "plt.title(\"ECDF\")\n",
    "plt.legend()\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plt.hist(onp.array(final_non_joint.flatten()), bins=100)\n",
    "for loc in locations: \n",
    "    plt.axvline(loc, color=\"black\", ls=\"--\")\n",
    "plt.xlabel(\"Support\")\n",
    "plt.title(\"Samples\")\n",
    "\n",
    "plt.sca(axes[2])\n",
    "plt.hist(onp.array(data.flatten()), bins=100)\n",
    "for loc in locations: \n",
    "    plt.axvline(loc, color=\"black\", ls=\"--\")\n",
    "plt.xlabel(\"Support\")\n",
    "plt.title(\"Data\")\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, yes, we're able to!\n",
    "Looking at the distributions,\n",
    "notice how the mixture weights are a bit different between the samples and the data.\n",
    "However, we should notice that the middle Gaussian has a long tail of samples.\n",
    "This implies something wrong with gradient estimation.\n",
    "The problem here is either due to a lack of training budget (2,000 steps only)\n",
    "or because it is genuinely difficult to estimate gradients in that regime.\n",
    "The latter is what Yang Song states as the main issue.\n",
    "\n",
    "Let's also do a side-by-side comparison of gradients for the original data \n",
    "v.s. the true score function of the mixture model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| fig-cap: Estimated gradients for the original data.\n",
    "#| label: fig-est-grads-original\n",
    "#| code-fold: true\n",
    "from score_models.models.gaussian import MixtureGaussian\n",
    "\n",
    "support = np.linspace(-20, 20, 1000).reshape(-1, 1)\n",
    "gradients = vmap(updated_model)(support)\n",
    "plt.plot(support, gradients, label=\"Estimated\")\n",
    "\n",
    "true_score_func = MixtureGaussian(\n",
    "    mus=np.array(locations),\n",
    "    log_sigmas=np.log(np.array([1.0, 1.0, 1.0])),\n",
    "    ws=np.array([0.5, 0.25, 0.25]),\n",
    ")\n",
    "original_gradients = vmap(true_score_func)(support).squeeze()\n",
    "plt.plot(support, original_gradients, label=\"True\")\n",
    "plt.legend()\n",
    "\n",
    "plt.title(\"Estimated Gradients\")\n",
    "plt.xlabel(\"Support\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.axhline(y=0, color=\"black\", ls=\"--\")\n",
    "plt.legend()\n",
    "sns.despine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients diagnostic here should also be quite illuminating.\n",
    "In particular, there are regions where the gradient estimation is way off.\n",
    "That is where the next point might come in handy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One score model per perturbation\n",
    "\n",
    "One key idea in Yang Song's blog post\n",
    "is that we can jointly train score models for each of the noise levels\n",
    "and then use Langevin dynamics in an annealed fashion\n",
    "to progressively obtain better and better samples from the original data distribution.\n",
    "The loss function here is a _weighted sum of Fisher divergences_,\n",
    "or, more simply, a sum of score model losses\n",
    "weighted by the noise scale applied to the data.\n",
    "The intuition here is that we weigh more heavily the strongly perturbed data\n",
    "and weigh less heavily the weakly perturbed data,\n",
    "because the score model will be more accurate for the strongly perturbed data.\n",
    "Thinking downstream two steps, we will be using a procedure called _annealed Langevin dynamics_\n",
    "to sample from this mixture Gaussian, such that \n",
    "In our example, we will have a batch of models trained with a single loss function,\n",
    "one for each scale value,\n",
    "which is the weighted sum of Fisher divergences,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we start with a `vmap`-ed version of our model.\n",
    "This will make it easy for us to train a batch of models together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyprojroot import here \n",
    "import cloudpickle as pkl\n",
    "\n",
    "def make_model(key):\n",
    "    ffmodel = FeedForwardModel1D(\n",
    "        depth=2, width_size=512, activation=nn.softplus, key=key\n",
    "    )\n",
    "    return ffmodel\n",
    "\n",
    "key = random.PRNGKey(49)\n",
    "keys = random.split(key, len(noise_scale))\n",
    "models = []\n",
    "for key in keys:\n",
    "    models.append(make_model(key))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our joint loss function.\n",
    "Here, the loss function is a weighted sum of score matching losses.\n",
    "In related body of work, the greater the noise scale, the higher the weight.\n",
    "The intuition here is that gradients are more accurately estimated at higher noise scales,\n",
    "while gradients are less accurately estimated at lower noise scales.\n",
    "For fairness in comparison, we will use the same number of training steps are before\n",
    "for independently-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models.losses import joint_score_matching_loss\n",
    "\n",
    "optimizer = optax.chain(\n",
    "    optax.clip(0.1),\n",
    "    optax.adam(5e-3),\n",
    ")\n",
    "\n",
    "(here() / \"artifacts\").mkdir(exist_ok=True)\n",
    "\n",
    "n_steps = 2_000\n",
    "\n",
    "artifact_path = here() / f\"artifacts/noise_scale_model_{n_steps}.pkl\"\n",
    "updated_models, training_history = fit(\n",
    "    models,\n",
    "    data_perturbed,\n",
    "    partial(joint_score_matching_loss, scales=noise_scale),\n",
    "    optimizer=optimizer,\n",
    "    steps=n_steps,\n",
    ")\n",
    "with open(artifact_path, \"wb\") as f:\n",
    "    pkl.dump((updated_models, training_history), f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm sampling works after joint training\n",
    "\n",
    "We're now going to do a quick sanity-check:\n",
    "our trained score models should be usable to sample from the mixture distribution.\n",
    "Let's confirm that before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_particles = 1_000\n",
    "starter_points = random.normal(key, shape=(n_particles, 1, 1)) * 10\n",
    "starter_keys = random.split(key, n_particles)\n",
    "trained_model_sampler = LangevinDynamicsChain(\n",
    "    gradient_func=updated_models[0], n_samples=100, epsilon=5e-1\n",
    ")\n",
    "final, trained_samples = vmap(trained_model_sampler)(starter_points, starter_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| fig-cap: Sanity check of sampling. (left) ECDF of samples vs. original data. Histogram of (middle) samples and (right) original data are also shown.\n",
    "#| label: fig-sampling-sanity-check.\n",
    "fig, axes = plt.subplots(figsize=(8, 3), ncols=3, nrows=1, sharex=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plt.plot(*ecdf(final.flatten()), label=\"Sampled\")\n",
    "plt.plot(*ecdf(data.flatten()), label=\"Original\")\n",
    "plt.xlabel(\"Support\")\n",
    "plt.title(\"ECDF\")\n",
    "plt.legend()\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plt.hist(onp.array(final.flatten()), bins=100)\n",
    "plt.xlabel(\"Support\")\n",
    "plt.title(\"Samples\")\n",
    "\n",
    "plt.sca(axes[2])\n",
    "plt.hist(onp.array(data.flatten()), bins=100)\n",
    "plt.xlabel(\"Support\")\n",
    "plt.title(\"Data\")\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annelaed Langevin Dynamics Sampling\n",
    "\n",
    "Annealed Langevin dynamics sampling is a way to get around the problem \n",
    "of poorly estimated gradients in low density regions.\n",
    "The procedure is rather simple and elegant.\n",
    "We start by performing Langevin dynamics sampling at the highest noise value.\n",
    "After a fixed number of steps,\n",
    "we freeze the samples and use them as the starting point\n",
    "for sampling at the next highest noise value,\n",
    "progressively stepping down the noise until we hit the unperturbed data.\n",
    "In doing so, we ensure that the score function can be progressively estimated\n",
    "around regions of high density\n",
    "while progressively worrying less and less about the low density gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_particles = 10_000\n",
    "starter_points = random.normal(key, shape=(n_particles, 1, 1)) * 10\n",
    "starter_keys = random.split(key, n_particles)\n",
    "\n",
    "final_points_history = []\n",
    "for model in updated_models[::-1]:\n",
    "    trained_model_sampler = LangevinDynamicsChain(\n",
    "        gradient_func=model, n_samples=100, epsilon=5e-1\n",
    "    )\n",
    "    final_points, trained_samples = vmap(trained_model_sampler)(starter_points, starter_keys)\n",
    "    final_points_history.append(final_points)\n",
    "    starter_points = final_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true \n",
    "#| fig-cap: Annealed Langevin dynamics samples at varying noise scales. Data (bottom-right) is shown for comparison.\n",
    "#| label: fig-annealed-langevin-dynamics-samples\n",
    "fig, axes = plt.subplots(figsize=(8, 4), ncols=5, nrows=2, sharey=False)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, history, scale in zip(axes, final_points_history, noise_scale[::-1]):\n",
    "    plt.sca(ax)\n",
    "    plt.hist(onp.array(history.flatten()), bins=100)\n",
    "    plt.title(f\"Scale: {scale:.1f}\")\n",
    "    for loc in locations:\n",
    "        plt.axvline(x=loc, ls=\"--\", color=\"black\")\n",
    "\n",
    "plt.sca(axes[-1])\n",
    "plt.hist(onp.array(data.flatten()), bins=100)\n",
    "plt.title(\"Data\")\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also compare samples taken from jointly trained vs. independently trained models\n",
    "alongside their estimated score functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| fig-cap: Comparison between true (top), independently trained (middle) and jointly trained (bottom) score models. (left) Score models evaluated along the support. (right) Samples drawn from true distribution (right-top) and by Langevin dynamics sampling (right middle and right bottom).\n",
    "#| label: fig-comparison\n",
    "support = np.linspace(-20, 20, 1000).reshape(-1, 1)\n",
    "\n",
    "fig, axes = plt.subplot_mosaic(\"\"\"\n",
    "AD\n",
    "AD\n",
    "BE\n",
    "BE\n",
    "CF\n",
    "CF\n",
    "\"\"\", figsize=(8, 8))\n",
    "# First subplot: show gradients for noise scale 1.0 and for ground truth.\n",
    "plt.sca(axes[\"A\"])\n",
    "true_score = vmap(true_score_func)(support.squeeze())\n",
    "plt.plot(support, true_score, label=\"True\")\n",
    "plt.title(\"True Score\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "plt.sca(axes[\"B\"])\n",
    "independent_score = vmap(updated_model)(support)\n",
    "plt.plot(support, independent_score, label=\"Independent\")\n",
    "plt.title(\"Independently-Trained\\nEstimated Score\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "plt.sca(axes[\"C\"])\n",
    "joint_score = vmap(updated_models[0])(support)\n",
    "plt.plot(support, joint_score, label=\"Joint\")\n",
    "plt.title(\"Jointly-Trained\\nEstimated Score\")\n",
    "plt.xlabel(\"Support\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "plt.sca(axes[\"D\"])\n",
    "plt.hist(onp.array(data.flatten()), bins=100)\n",
    "plt.title(\"True Distribution Samples\")\n",
    "\n",
    "plt.sca(axes[\"E\"])\n",
    "plt.hist(onp.array(final_non_joint.flatten()), bins=100)\n",
    "plt.title(\"Independently-Trained Samples\")\n",
    "\n",
    "plt.sca(axes[\"F\"])\n",
    "plt.hist(onp.array(final_points_history[-1].flatten()), bins=100)\n",
    "plt.title(\"Annealed Langevin\\nDynamics Samples\")\n",
    "plt.xlabel(\"Support\")\n",
    "\n",
    "for axname, ax in axes.items():\n",
    "    plt.sca(ax)\n",
    "    for loc in locations:\n",
    "        plt.axvline(x=loc, ls=\"--\", color=\"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it's tempting to look at the peaks, the region of low density is where we should focus our attention.\n",
    "In the middle row's samples, we see that the independently-trained model\n",
    "has a long tail of density between the middle component and the right component,\n",
    "something that is missing in the true distribution's draws and in the annealed samples.\n",
    "The same can be seen on the left side, where we see the estimated score function\n",
    "taking on a rather inaccurate shape in that low density regime."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "499aa38474d161e044ebb3be9240784e1719d4331ad512ef6546dcd230708004"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('score-models')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
