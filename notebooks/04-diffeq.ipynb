{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false \n",
    "#| output: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import os \n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "import jax.numpy as np  # import here so that any warnings about no GPU are not shown in website.\n",
    "np.arange(3)\n",
    "import shutup\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential Equations\n",
    "\n",
    "We're going to take a small detour through ordinary differential equations (ODEs).\n",
    "ODEs, and their stochastic counterparts, Stochastic Differential Equations (SDEs),\n",
    "are an important technical advance in score-based generative modelling.\n",
    "In this section, we're going to see how to use SDEs to noise up data,\n",
    "thereby replacing the noising step,\n",
    "and reverse-time versions of the SDEs to generate new data,\n",
    "thereby replacing the annealed Langevin dynamics step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary differential equations\n",
    "\n",
    "Let's start with ODEs.\n",
    "ODEs are usually taught in undergraduate calculus classes,\n",
    "since they involve differentiation and integration.\n",
    "I do remember encountering them \n",
    "while studying in secondary school and junior college in Singapore,\n",
    "which is a testament to how advanced the mathematics curriculum in Singapore is.\n",
    "\n",
    "ODEs are useful models of systems\n",
    "where we believe that the rate of change of an output variable\n",
    "is a math function of some input variable.\n",
    "In abstract mathematical symbols:\n",
    "\n",
    "$$\\frac{dy}{dx} = f(x, \\theta)$$\n",
    "\n",
    "Here, $f$ simply refers to some mathematical function of $x$\n",
    "and the function's parameters $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A classic ODE example\n",
    "\n",
    "A classic ODE example that we might think of is that of a decay curve:\n",
    "\n",
    "$$\\frac{dy}{dt} = -y$$\n",
    "\n",
    "Implemented in `diffrax`, which is a JAX package for differential equations,\n",
    "and wrapped in Equinox as a parameterized function,\n",
    "we have the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffrax import diffeqsolve, Tsit5, ODETerm, SaveAt\n",
    "import jax.numpy as np\n",
    "from jax import vmap\n",
    "import equinox as eqx\n",
    "\n",
    "def exponential_decay_drift(t: float, y: float, args: tuple):\n",
    "    \"\"\"Exponential decay drift term.\"\"\"\n",
    "    return -y\n",
    "\n",
    "\n",
    "from score_models.models.ode import ODE\n",
    "from inspect import getsource \n",
    "\n",
    "print(getsource(ODE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those of us who have learned about ODEs, \n",
    "the structure of the code above should look pretty familiar.\n",
    "The diffrax API neatly organizes what we need to solve ODEs:\n",
    "\n",
    "- the `ODETerm`, which is the $\\frac{dy}{dt}$ equation,\n",
    "- a `solver`, for which `diffrax` provides a library of them,\n",
    "- the initial and end points $t_0$ and $t_1$ along the $t$ axis along with step size $dt$,\n",
    "- the initial value of $y$, i.e. $y_0$.\n",
    "\n",
    "Finally, when calling on the ODE,\n",
    "we evaluate the solution path from the starting time to the ending time,\n",
    "given an initial starting value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ode = ODE(exponential_decay_drift)\n",
    "ts = np.linspace(0, 10, 1000)\n",
    "ys = ode(ts=ts, y0=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true \n",
    "#| fig-cap: Solution to the ODE $f'(y) = -y$.\n",
    "#| label: fig-ode-exponential-decay\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "plt.plot(ts, ys)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y\")\n",
    "sns.despine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution of the ODE that we had above is an exponential decay,\n",
    "and that is exactly what we see in the curve above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we wanted to run the ODE from multiple starting points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| fig-cap: Multiple solutions to the ODE $f'(y) = -y$.\n",
    "#| label: fig-ode-multiple-decay\n",
    "\n",
    "ys = ode(ts=ts, y0=np.arange(-10, 10))\n",
    "\n",
    "for curve in ys.T:\n",
    "    plt.plot(ts, curve, color=\"blue\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Differential Equations\n",
    "\n",
    "Stochastic differential equations (SDEs) extend ODEs\n",
    "by adding in noise into each step. \n",
    "SDEs can thus be thought of as having a \"drift\" component,\n",
    "in which the system being modeled by the SDE \"drifts\" through the vector field,\n",
    "and a \"diffusion\" component,\n",
    "in which the system's state is perturbed with additional noise.\n",
    "SDEs have the general form:\n",
    "\n",
    "$$dx = f(x, t)dt + g(t)dw$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To paraphrase Yang's blog post, here are the definitions of each of the terms.\n",
    "\n",
    "- $f(x, t)$ is a drift function that produces a vector output, \n",
    "  i.e. what would have been the ODE term. \n",
    "  This term controls the \"drift\"-ing of the system in observed data space.\n",
    "- $g(t)$ is a diffusion function that produces a scalar output, \n",
    "  i.e. the scalar multiplier of $dw$.\n",
    "  This term adds \"diffusive\" noise to the output.\n",
    "- $dw$ is the infinitesimal white noise term.\n",
    "\n",
    "$f(x, t)dt$ is usually referred to as the \"ODE Term\",\n",
    "while $g(t)dw$ is usually referred to as the \"Control Term\".\n",
    "We can see that in the implementation of the SDE module below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models.models.sde import SDE\n",
    "\n",
    "print(getsource(SDE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisy Decay\n",
    "\n",
    "For illustration, \n",
    "let's see what happens we we apply homoskedastic noise to the decay process.\n",
    "Here, homoskedastic noise refers to a noise term that is _independent_ of time.\n",
    "Firstly, we have it defined in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def homoskedastic_diffusion(t, y, args):\n",
    "    \"\"\"Time-independent noise.\"\"\"\n",
    "    return 0.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set up the SDE and solve it going forward in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random \n",
    "\n",
    "n_timesteps = 17\n",
    "n_starting = 1001\n",
    "\n",
    "demo_key = random.PRNGKey(55)\n",
    "\n",
    "y0_key, key = random.split(demo_key)\n",
    "y0s = random.normal(y0_key, shape=(n_starting,))  # We solve the SDE for each draw from a Guassian.\n",
    "\n",
    "sde_keys = random.split(key, len(y0s))\n",
    "ts = np.linspace(0, 4, n_timesteps)\n",
    "sde = SDE(drift=exponential_decay_drift, diffusion=homoskedastic_diffusion)\n",
    "sde = partial(sde, ts)\n",
    "ys = vmap(sde)(y0s, sde_keys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the trajectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| label: fig-sde-decay-homoskedastic\n",
    "#| fig-cap: SDE with exponential decay drift and homoskedastic disffusion.\n",
    "for y in ys:\n",
    "    plt.plot(ts, y, alpha=0.01, color=\"blue\")\n",
    "\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oscillating SDE \n",
    "\n",
    "Let's do another example: oscillating SDEs!\n",
    "Here, we have an oscillating system (cosine drift)\n",
    "in which we add a homoskedastic diffusion term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_drift(t, y, args):\n",
    "    return np.cos(t)\n",
    "\n",
    "\n",
    "sde = SDE(drift=cosine_drift, diffusion=homoskedastic_diffusion)\n",
    "ts_oscillating = np.linspace(1, 10, n_timesteps)\n",
    "sde = partial(sde, ts_oscillating)\n",
    "keys = random.split(key, 1001)\n",
    "oscillating_y0s = random.normal(key, shape=(1001,)) * 0.1\n",
    "oscillating_ys = vmap(sde)(oscillating_y0s, keys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, let's plot this one too:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| label: fig-sde-oscillating-homoskedastic\n",
    "#| fig-cap: SDE with exponential decay drift and homoskedastic disffusion.\n",
    "\n",
    "for y in oscillating_ys:\n",
    "    plt.plot(ts_oscillating, y, color=\"blue\", alpha=0.01)\n",
    "\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y\")\n",
    "sns.despine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each timepoint, there is also a marginal distribution.\n",
    "Let's inspect that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| label: fig-sde-oscillating-marginals\n",
    "#| fig-cap: Marginal distribution at each time point of the oscillating SDE.\n",
    "\n",
    "import numpy as onp\n",
    "fig, axes = plt.subplots(figsize=(8, 10), nrows=6, ncols=3, sharex=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, t, y in zip(axes, ts_oscillating, oscillating_ys.T):\n",
    "    plt.sca(ax)\n",
    "    plt.hist(onp.array(y), bins=30)\n",
    "    plt.title(f\"time={t:.1f}\")\n",
    "\n",
    "sns.despine()\n",
    "plt.delaxes(axes[-1])\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noising SDE\n",
    "\n",
    "For the purposes of noising up date,\n",
    "we would want an SDE that noises up data with increasing amounts of noise with time.\n",
    "Here, we can design the SDE such that the drift would be 0 at all time points,\n",
    "while the diffusion term would be some multiplier on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constant_drift(t, y, args):\n",
    "    \"\"\"Constant drift term.\"\"\"\n",
    "    return 0\n",
    "\n",
    "def time_dependent_diffusion(t, y, args):\n",
    "    \"\"\"Diffusion term that increases with time.\"\"\"\n",
    "    return 0.3 * t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we set up the SDE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sde = SDE(drift=constant_drift, diffusion=time_dependent_diffusion)\n",
    "ts_noising = np.linspace(0, 4, n_timesteps)\n",
    "sde = partial(sde, ts_noising)\n",
    "y0s = random.normal(key, shape=(n_starting,)) * 0.1  # we start with N(0, 0.1) draws.\n",
    "keys = random.split(key, n_starting)\n",
    "noising_ys = vmap(sde)(y0s, keys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's plot the solved SDE trajectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| label: fig-noising-sde\n",
    "#| fig-cap: A \"noising\" SDE that progressively adds more noise over time.\n",
    "for y in noising_ys:\n",
    "    plt.plot(ts_noising, y, color=\"blue\", alpha=0.01)\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.title(f\"{n_starting} sample trajectories\")\n",
    "    sns.despine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, \n",
    "we are able to obtain greater amounts of noise from a tight starting point.\n",
    "We can verify that by looking at the marginal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| label: fig-sde-noising-marginals\n",
    "#| fig-cap: Marginal distribution at each time point of a noising SDE.\n",
    "\n",
    "import numpy as onp\n",
    "fig, axes = plt.subplots(figsize=(8, 10), nrows=6, ncols=3, sharex=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, t, y in zip(axes, ts_noising, noising_ys.T):\n",
    "    plt.sca(ax)\n",
    "    plt.hist(onp.array(y), bins=30)\n",
    "    plt.title(f\"time={t:.1f}, σ={onp.std(y):.2f}\")\n",
    "\n",
    "sns.despine()\n",
    "plt.delaxes(axes[-1])\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the marginal distributions at each noise timestep,\n",
    "we see that we indeed have ever increasing amounts of noise.\n",
    "(Note how the x-axis scale is the same on all of the plots.)\n",
    "The empirical standard deviation from the mean is also shown on the plots above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse Time SDEs\n",
    "\n",
    "With constant drift and time-dependent diffusion,\n",
    "we can noise up data in a continuous fashion.\n",
    "How do we go backwards?\n",
    "Here is where solving the reverse time SDE will come in. \n",
    "Again, we need to set up the drift and diffusion terms.\n",
    "Here, the drift term is:\n",
    "\n",
    "$$f(x, t) - g^2(t) \\nabla_x \\log p_t (x) $$\n",
    "\n",
    "where:\n",
    "\n",
    "- $f(x, t)$ is the drift term of the forward SDE,\n",
    "- $g(t)$ is the diffusion term of the forward SDE, and\n",
    "- $\\nabla_x \\log p_t (x)$ is the score function of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "And the diffusion term is:\n",
    "\n",
    "$$g(t) dw$$\n",
    "\n",
    "which is basically the diffusion term of the forward SDE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the tricky part here is that we don't have access to\n",
    "$\\nabla_x \\log p_t (x)$ (the true score function).\n",
    "As such, we need to bring out our score model approximator!\n",
    "To train the score model approximator,\n",
    "we need the analogous score matching objective for continuous time problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Time Score Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an ideal situation,\n",
    "we would train the score matching model\n",
    "using a weighted combination of Fisher divergences:\n",
    "\n",
    "$$\\mathbb{E}_{t \\in U(0, T)} \\mathbb{E}_{p_t(x)} [ \\lambda(t) || \\nabla_x \\log p_t(x) - s_{\\theta}(x, t) ||^2_2]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, just like before, we don't have access to $\\nabla_x \\log p_t (x)$,\n",
    "so we instead use the score matching objective by Hyvärinen [@JMLR:v6:hyvarinen05a].\n",
    "What's really cool here is that we can train the models using the noised up data.\n",
    "The protocol is basically as follows:\n",
    "\n",
    "1. Noise up our original data using an SDE.\n",
    "2. Train score models to estimate the score function of the noised up data.\n",
    "3. Use the approximate score function to calculate the reverse-time SDE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model implementation\n",
    "\n",
    "To get this right, we need a score function approximator that is compatible with SDEs,\n",
    "i.e. they accept both `x` and `t` as part of the function signature\n",
    "and return the gradient value.\n",
    "\n",
    "```python\n",
    "score: float = score_model(x, t) \n",
    "```\n",
    "\n",
    "Let's implement it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jacfwd\n",
    "from jax import nn\n",
    "import equinox as eqx\n",
    "\n",
    "class SDEFeedForwardModel1D(eqx.Module):\n",
    "    \"\"\"Time-dependent score model.\n",
    "    \n",
    "    We choose an MLP here with 2 inputs (`x` and `t` concatenated),\n",
    "    and output a scalar which is the estimated score.\n",
    "    \"\"\"\n",
    "\n",
    "    mlp: eqx.Module\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_size=2,\n",
    "        out_size=1,\n",
    "        width_size=256,\n",
    "        depth=1,\n",
    "        activation=nn.softplus,\n",
    "        key=random.PRNGKey(45),\n",
    "    ):\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            in_size=in_size,\n",
    "            out_size=out_size,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            activation=activation,\n",
    "            key=key,\n",
    "        )\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x: float, t: float):\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        :param x: Data. Should be of shape (1, :),\n",
    "            as the model is intended to be vmapped over batches of data.\n",
    "        :returns: Estimated score of a Gaussian.\n",
    "        \"\"\"\n",
    "        if isinstance(x, float) or x.ndim == 0:\n",
    "            x = np.array([x])\n",
    "        if isinstance(t, float) or x.ndim == 0:\n",
    "            t = np.array([t])\n",
    "        x = np.array([x.squeeze(), t.squeeze()])\n",
    "        return self.mlp(x).squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few design notes for the model above that we'd like to note.\n",
    "\n",
    "Firstly, note how its structure \n",
    "is essentially identical to the neural net score model from before,\n",
    "i.e. a multi-layer perceptron,\n",
    "except that now it takes in both `x` and `t` as its inputs.\n",
    "This is important because we are no longer interested in a discrete score model,\n",
    "with one per time point.\n",
    "Instead, we are interested in a score model \n",
    "that can estimate the score function of our noised up data \n",
    "at any time point along the SDE-based continuous-time noising function.\n",
    "\n",
    "Let's now instantiate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SDEFeedForwardModel1D(\n",
    "    width_size=256, depth=2, activation=nn.softplus, key=random.PRNGKey(55)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "Now, we need to calculate the loss for a batch of data.\n",
    "As with before, we need a score-matching loss for each noise level\n",
    "(i.e. for each `t`).\n",
    "In here, we have the model's dependence on time (i.e. noise level)\n",
    "encoded as part of the model structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models.losses.sde import score_matching_loss as sde_score_matching_loss, joint_score_matching_loss as sde_joint_score_matching_loss\n",
    "\n",
    "print(getsource(sde_score_matching_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(getsource(sde_joint_score_matching_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train score model\n",
    "\n",
    "Now, let's train the score model on our noised up data.\n",
    "Below, we have our training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from score_models.losses import joint_sde_score_matching_loss, sde_score_matching_loss\n",
    "\n",
    "model = SDEFeedForwardModel1D(key=random.PRNGKey(55))\n",
    "\n",
    "optimizer = optax.chain(\n",
    "    optax.adam(5e-4),\n",
    ")\n",
    "\n",
    "opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
    "dloss = eqx.filter_jit(eqx.filter_value_and_grad(joint_sde_score_matching_loss))\n",
    "\n",
    "n_steps = 13_000\n",
    "iterator = tqdm(range(n_steps))\n",
    "loss_history = []\n",
    "updated_score_model = model\n",
    "for step in iterator:\n",
    "    loss_score, grads = dloss(updated_score_model, noising_ys.T, ts_noising)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    updated_score_model = eqx.apply_updates(updated_score_model, updates)\n",
    "    iterator.set_description(f\"Score: {loss_score:.2f}\")\n",
    "    loss_history.append(float(loss_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also make sure that the model training has converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true \n",
    "#| fig-cap: Training loss curve for our continuous-time score model.\n",
    "#| label: fig-training-loss-curve\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss Score\")\n",
    "sns.despine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity-check: score estimators match up with alternate calculation\n",
    "\n",
    "Because we started with Gaussian noise and expanded the noise outwards,\n",
    "we still have Gaussians.\n",
    "Let's check that the scores match a Gaussian's score \n",
    "fitted onto the marginal distributions at each timepoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| fig-cap: Estimated (blue) vs. approximated (red) score functions at each time evaluation. Estimated score comes from taking the location (mean) and scale (stdev) of the observed data, while approximated score comes from the time-based score model.\n",
    "#| label: fig-score-vs-time\n",
    "from jax.scipy.stats import norm \n",
    "from jax import grad \n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=6, ncols=3, figsize=(8.5, 11))\n",
    "\n",
    "for t, noised_ys, ax in zip(ts_noising, noising_ys.T, axes.flatten()):\n",
    "    plt.sca(ax)\n",
    "    noised_ys_mu = np.mean(noised_ys)\n",
    "    noised_ys_sigma = np.std(noised_ys)\n",
    "\n",
    "    logp_func = partial(norm.logpdf, loc=noised_ys_mu, scale=noised_ys_sigma)\n",
    "    np.sum(logp_func(noised_ys))\n",
    "    dlogp_func = grad(logp_func)\n",
    "\n",
    "    support = np.linspace(noised_ys_mu - noised_ys_sigma * 3, noised_ys_mu + noised_ys_sigma * 3, 1000)\n",
    "    estimated_score = vmap(dlogp_func)(support)\n",
    "    approximated_score = vmap(partial(updated_score_model, t=t))(support)\n",
    "    plt.plot(support, estimated_score, color=\"blue\", label=\"estimated\")\n",
    "    plt.plot(support, approximated_score, color=\"red\", label=\"approximated\")\n",
    "    plt.xlabel(\"Support\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(f\"t={t:.2f}\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.sca(axes.flatten()[0])\n",
    "plt.legend()\n",
    "sns.despine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in @fig-score-vs-time,\n",
    "it looks like our score model is able to approximate a time-dependent score function!\n",
    "The score function is least well-approximated within the region of 2 sigmas of support,\n",
    "even if not across the full 3 sigmas.\n",
    "This is encouraging.\n",
    "We should also note that the `t=4.00` timepoint is the least well-approximated\n",
    "compared to the `t=1.00` timepoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're going to look at the reverse drift.\n",
    "In an SDE, the drift term dictates \n",
    "where the system is going to move towards in the next time step.\n",
    "Let's plot the vector field evaluated at each time step `t`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_drift(t: float, y: float, args: tuple):\n",
    "    f = constant_drift(t, y, args)\n",
    "    g = time_dependent_diffusion(t, y, args)\n",
    "    s = updated_score_model(y, t)\n",
    "    return f - 0.5 * g**2 * s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot each of these four terms to make sure we get a good feel for what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| label: fig-reverse-sde-components\n",
    "#| fig-cap: Heatmap of each of the component functions in reverse_drift.\n",
    "\n",
    "import pandas as pd \n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "# Plot constant drift as a function of y and t.\n",
    "ys = np.linspace(-3, 3, 50)\n",
    "ts = np.linspace(0, 5, 50)\n",
    "\n",
    "function_evals = []\n",
    "for yval in ys:\n",
    "    for t in ts:\n",
    "        dd = dict()\n",
    "        dd[\"constant_drift\"] = constant_drift(y=yval, t=t, args=())\n",
    "        dd[\"time_dependent_diffusion\"] = time_dependent_diffusion(y=yval, t=t, args=())\n",
    "        dd[\"score_approximation\"] = updated_score_model(x=yval, t=t)\n",
    "        dd[\"reverse_drift\"] = reverse_drift(y=yval, t=t, args=())\n",
    "        dd[\"y\"] = (yval)\n",
    "        dd[\"t\"] = t \n",
    "        function_evals.append(dd)\n",
    "\n",
    "columns = [\"constant_drift\", \"time_dependent_diffusion\", \"score_approximation\", \"reverse_drift\"]\n",
    "\n",
    "function_df = pd.DataFrame(function_evals)\n",
    "for column in function_df.columns:\n",
    "    function_df[column] = function_df[column].astype(float)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(8, 8))\n",
    "for ax, col in zip(axes.flatten(), columns):\n",
    "    function_eval = function_df[[\"y\", \"t\", col]].pivot_table(index=\"y\", columns=\"t\", values=col)\n",
    "    sns.heatmap(function_eval, ax=ax, cmap=\"viridis\")\n",
    "    ax.set_title(col)\n",
    "    ax.set_xticklabels([f\"{float(i._text):.2f}\" for i in ax.get_xticklabels()])\n",
    "    ax.set_yticklabels([f\"{float(i._text):.2f}\" for i in ax.get_yticklabels()])\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `constant_drift` is always 0, so no problem.\n",
    "- `time_dependent_diffusion` shows how diffusion increases over time, independent of the value of `y`, which is also correct, so no problem.\n",
    "- `score_approximation` shows how the Gaussian score approximator gives a gradient that is positive-valued when `y` is negative and vice versa, which pushes us towards region of high density. Also correct.\n",
    "- `reverse_drift` shows us something interesting. We will end up with exploding values because +ve values drift more positive, while -ve values drift more -ve, until we hit very small time steps, and then we have no directional drift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we could, in theory, run the SDE in reverse,\n",
    "but in my own testing of the equations, \n",
    "I found that I would encounter numerical stability issues.\n",
    "Because of the positive and negative drift zones in the reverse drift,\n",
    "we would end up getting extremely large negative or positive numbers.\n",
    "Hence, I skipped over solving the reverse SDE \n",
    "and instead went straight to probability flow ODEs,\n",
    "which are the very, very exciting piece of this entire body of work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Flow ODEs\n",
    "\n",
    "Now that we've recapped what an ODE is, \n",
    "and have seen what SDEs can do for noising up our data,\n",
    "we can move on to probability flow ODEs.\n",
    "Why are these important? \n",
    "It's because probability flow ODEs \n",
    "provide a deterministic mapping from our noise distribution to our data distribution\n",
    "and vice versa.\n",
    "In doing so, we can basically get rid of Langevin dynamics sampling\n",
    "and replace it entirely with a probability flow ODE instead.\n",
    "Before we go on, though, let's take a quick look the key ODE that we need to solve:\n",
    "\n",
    "$$dx = [f(x,t) - \\frac{1}{2} g^2(t) \\nabla_x \\log p_t (x)] dt$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the SDE above, the terms carry the same meaning:\n",
    "\n",
    "> - $f(x, t)$ is a drift function that produces a vector output,\n",
    "> - $g(t)$ is a diffusion function that produces a scalar output,\n",
    "> - $\\nabla_x \\log p_t (x)$ is the score function, also replaceable by our neural net approximator\n",
    "> - and $dw$ is infinitesimal white noise.  \n",
    "> \n",
    "> (paraphrased from Yang's blog)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you study this carefully... \n",
    "the drift term is _exactly_ the drift term we defined above!\n",
    "That means if we solve for the reverse drift ODE,\n",
    "we will get a path traced from the noise distribution\n",
    "back to the orginal data distribution!\n",
    "Let's see that in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| fig-cap: Probability flow ODE from noise distribution (t=5) back to original data (t=0).\n",
    "#| label: fig-prob-flow-ode\n",
    "#| code-fold: true\n",
    "\n",
    "ode_combined = ODE(reverse_drift)\n",
    "\n",
    "ode = ode_combined\n",
    "ts = np.linspace(5, 0, 1000)\n",
    "key = random.PRNGKey(55)\n",
    "y0s = np.linspace(-5 ,5, 10)\n",
    "\n",
    "for y0 in y0s:\n",
    "    ys = ode(ts, y0)\n",
    "    plt.plot(ts, ys)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.gca().invert_xaxis()\n",
    "sns.despine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just like that, we have:\n",
    "\n",
    "- An SDE that noises up data (forward-time),\n",
    "- A score model that estimates the score of the data, and\n",
    "- A probability flow ODE that maps from noise back to data (reverse-time).\n",
    "\n",
    "That last point is the coolest of them all, in my opinion.\n",
    "Previously, we used Langevin dynamics sampling to sample out new sequences.\n",
    "While random sampling is simple,\n",
    "it also aesthetically felt less elegant than what we have with an ODE.\n",
    "The key ingredients here are:\n",
    "\n",
    "- A time-dependent score model that can calculate (or estimate) the score of our noised distribution,\n",
    "- The known noise generator \"diffusion\" function, and\n",
    "- The known drift function,\n",
    "\n",
    "and all we need to do is solve a neural ODE while reversing time.\n",
    "Then, by drawing new coordinates from the noise distribution,\n",
    "we can deterministically map them back to the original data space!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "499aa38474d161e044ebb3be9240784e1719d4331ad512ef6546dcd230708004"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('score-models')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
