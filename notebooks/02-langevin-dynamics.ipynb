{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false \n",
    "#| output: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import os \n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "import jax.numpy as np  # import here so that any warnings about no GPU are not shown in website.\n",
    "np.arange(3)\n",
    "import shutup\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langevin Dynamics\n",
    "\n",
    "In the previous chapter, \n",
    "we explored how neural networks can be used \n",
    "_to approximate the score function_ of a data-generating distribution.\n",
    "In doing so, we obtain the gradient of the log density of the data generator.\n",
    "How can we use this gradient information?\n",
    "That is what we're going to explore in this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from a density without knowing the density\n",
    "\n",
    "As mentioned in the first chapter,\n",
    "one of the key motivations in using score models \n",
    "is to generate new data samples from existing samples.\n",
    "In the case of data such as images, audio, text, and other complicated modalities,\n",
    "the data generating distribution _can't_ be written down in some analytical form.\n",
    "In other words, complex data (images, audio, text, etc.) come from an _unknown_ density.\n",
    "So how do we draw samples from that distribution that are similar to existing samples\n",
    "without having access to the actual density?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That situation is exactly where having an estimator of the score function is important!\n",
    "By estimating the score function using existing data,\n",
    "we can use score function approximator \n",
    "to guide us to another set of coordinates in the input space,\n",
    "thus yielding a new sample drawn from the data-generating density.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling implies not simply following gradients naÃ¯vely.\n",
    "In other words, we're not merely interested\n",
    "in following the gradients to another high likelihood position.\n",
    "Rather, sampling implies the use of stochasticity.\n",
    "One sampling strategy that provides us with gradients and stochasticity\n",
    "is called \"Langevin dynamics\".\n",
    "Let's explore what it is over here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langevin dynamics, the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Yang Song's blog,\n",
    "\n",
    "> Langevin dynamics provides an MCMC procedure to sample from a distribution\n",
    "> $p(x)$ using only its score function $\\nabla_x \\log p(x)$. \n",
    "> Specifically, it initializes the chain from an arbitrary prior distribution\n",
    "> $x_0 \\sim \\pi(x)$, and then iterates the following:\n",
    ">\n",
    "> $$x_{i+1} \\leftarrow x_i + \\epsilon \\nabla_x \\log p(x) + \\sqrt{2 \\epsilon} z_i$$\n",
    "\n",
    "where $i = 0, 1, ... K$\n",
    "and $z_i \\sim \\text{Normal}(0, I)$\n",
    "is a multivariate Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dissect each term in the equation above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $x_i, x_{i+1}, ...$ refer to the draws that are sampled out of the procedure at each iteration $i$.\n",
    "- $\\nabla_x \\log p(x)$ is the gradient of the logp of the density w.r.t. $x$. This is exactly the score function that we're trying to approximate with our models. This term gives us a step in the direction of the gradient.\n",
    "- $\\sqrt{2 \\epsilon}z_i$ is a term that injects noise into the procedure.\n",
    "- $\\epsilon$ is a scaling factor, akin to a hyperparameter, that lets us control the magnitude of the step in the gradient direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you probably can see, we basically start at some point $x_i$ in the input space $x$,\n",
    "use the score function to move in a direction,\n",
    "but done with the injection of noise into the procedure to make it a stochastic procedure.\n",
    "As such, the new value $x_{i+1}$ that we draw will be a value from the distribution $P(x)$,\n",
    "but biased towards higher estimated densities by nature of following the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langevin dynamics, in Python\n",
    "\n",
    "Let's see how that one Langevin dynamics step might be translated into Python:\n",
    "\n",
    "```python\n",
    "from jax import random, numpy as np\n",
    "\n",
    "def langevin_dynamics_step(prev_x, score_func, epsilon, key):\n",
    "    \"\"\"One step of Langevin dynamics sampling.\"\"\"\n",
    "    draw = random.normal(key)\n",
    "    new_x = prev_x + epsilon * score_func(prev_x) + np.sqrt(2 * epsilon) * draw\n",
    "    return new_x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A worked example with 1D univarite Gaussians\n",
    "\n",
    "Let's walk through a worked example that uses 1D Normal distributions.\n",
    "We will start with a mixture Gaussian distribution that has two components,\n",
    "estimate the score function of the mixture Gaussian using a neural network,\n",
    "and then use the score function to do sampling of new draws from the Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false \n",
    "#| output: false\n",
    "import jax.numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "#| label: fig-data\n",
    "#| fig-cap: Empirical cumulative distribution function (ECDF) and histogram of a 2-component mixture Gaussian data.\n",
    "#| code-fold: true\n",
    "\n",
    "from jax import random \n",
    "import jax.numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as onp\n",
    "\n",
    "def ecdf(data):\n",
    "    x, y = np.sort(data), np.arange(1, len(data) + 1) / len(data)\n",
    "    return x, y\n",
    "\n",
    "key = random.PRNGKey(45)\n",
    "k1, k2, k3 = random.split(key, 3)\n",
    "\n",
    "mix1 = random.normal(k1, shape=(1000,)) * 3 - 5\n",
    "mix2 = random.normal(k2, shape=(500,)) * 1 + 6\n",
    "data = np.concatenate([mix1, mix2]).reshape(-1, 1)\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(8, 4), ncols=2)\n",
    "plt.sca(axes[0])\n",
    "plt.plot(*ecdf(data.flatten()))\n",
    "plt.xlabel(\"Support\")\n",
    "plt.ylabel(\"Cumulative Distribution\")\n",
    "plt.title(\"ECDF\")\n",
    "sns.despine()\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plt.hist(onp.array(data), bins=100)\n",
    "plt.xlabel(\"Support\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a score function model\n",
    "\n",
    "As with before, we will train an approximate score function \n",
    "on this mixture Gaussian data.\n",
    "The model architecture will be a simple feed-forward neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models.training import fit\n",
    "from score_models.models import FeedForwardModel1D\n",
    "from score_models.losses import score_matching_loss\n",
    "import optax\n",
    "\n",
    "ffmodel = FeedForwardModel1D()\n",
    "\n",
    "optimizer = optax.adam(learning_rate=5e-3)\n",
    "updated_model, loss_history = fit(\n",
    "    ffmodel, \n",
    "    data, \n",
    "    score_matching_loss, \n",
    "    optimizer, \n",
    "    steps=2_000, \n",
    "    progress_bar=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now diagnose whether we converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: fig-gaussian-model-loss-history\n",
    "#| fig-cap: \"Loss curve for Gaussian score model.\"\n",
    "#| code-fold: true\n",
    "from jax import vmap\n",
    "fig, axes = plt.subplots(figsize=(8, 4), ncols=2)\n",
    "\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Training Iteration\")\n",
    "plt.ylabel(\"Score Matching Loss\")\n",
    "plt.title(\"Score Matching Loss History\")\n",
    "sns.despine()\n",
    "\n",
    "plt.sca(axes[1])\n",
    "updated_model_scores = vmap(updated_model)(data)\n",
    "plt.scatter(data.squeeze(), updated_model_scores.squeeze())\n",
    "plt.xlabel(\"Support\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Estimated Scores\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what we know about how the score function of a 2-component mixture should look like,\n",
    "It is safe to say that we have converged and can use the trained model.\n",
    "One thing should be noted here:\n",
    "we have explicitly avoided doing train/val/test splits here, but doing so is recommended!\n",
    "Just as with any other loss function for predicting classes or real numbers,\n",
    "we would use splitting here to determine when to stop training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample using the score function\n",
    "\n",
    "We are now going to attempt to use the neural network score approximator\n",
    "in a Langevin dynamics MCMC sampler.\n",
    "Langevin dynamics, being an iterative MCMC sampler,\n",
    "needs the use of a for-loop with carryover construct.\n",
    "I have taken advantage of `jax.lax.scan` for fast, compiled looping with carryover.\n",
    "In addition to that, because the operation requires parameterization of a function,\n",
    "Equinox is another natural choice for its implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "from score_models.sampler import LangevinDynamicsChain\n",
    "from inspect import getsource\n",
    "\n",
    "print(getsource(LangevinDynamicsChain))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample one chain\n",
    "\n",
    "Let's run one chain of the Langevin dynamics sampler\n",
    "to see what the samples from one chain look like. \n",
    "For comparison, we will show what the sampler draws look like\n",
    "when we have an untrained model vs. a trained model,\n",
    "and so we will have two samplers instantiated as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_sampler = LangevinDynamicsChain(gradient_func=updated_model, epsilon=5e-1)\n",
    "key = random.PRNGKey(55)\n",
    "final, trained_samples = trained_model_sampler(np.array([[2.0]]), key)\n",
    "\n",
    "untrained_model_sampler = LangevinDynamicsChain(gradient_func=ffmodel, epsilon=5e-1)\n",
    "final, untrained_samples = untrained_model_sampler(np.array([[2.0]]), key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the Langevin dynamics samplers have been instantiated and run for one chain,\n",
    "let's see what our \"draws\" look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| label: fig-langevin-dynamics-trained-vs-untrained\n",
    "#| fig-cap: Draws from Langevin dynamics sampler. Samples from a sampler with an (a) untrained neural network score function and (b) trained neural network compared to (c) original sampled data.\n",
    "fig, axes = plt.subplots(figsize=(8, 2.5), ncols=3, sharex=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plt.hist(onp.array(untrained_samples), bins=100)\n",
    "plt.title(\"(a) Untrained\")\n",
    "plt.xlabel(\"Support\")\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plt.hist(onp.array(trained_samples), bins=100)\n",
    "plt.title(\"(b) Trained\")\n",
    "plt.xlabel(\"Support\")\n",
    "\n",
    "plt.sca(axes[2])\n",
    "plt.hist(onp.array(data), bins=100)\n",
    "plt.title(\"(c) Original data\")\n",
    "plt.xlabel(\"Support\")\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks amazing!\n",
    "It looks abundantly clear to me that with one chain, \n",
    "we can draw new samples from our mixture distribution\n",
    "_without needing to know the mixture distribution parameters_!\n",
    "There isn't perfect correspondence,\n",
    "but for the purposes of drawing new samples _that look like existing ones_,\n",
    "an approximate model appears to be good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Chain Sampling\n",
    "\n",
    "We are now going to attempt multi-chain sampling!\n",
    "Let us instantiate 1,000 starter points \n",
    "drawn randomly from a Gaussian\n",
    "and then run the sampler for 200 steps.\n",
    "Note here that by designing our single chain sampler \n",
    "to be called on a single starter point (of the right shape)\n",
    "and a pseudorandom number generator key,\n",
    "we can `vmap` the sampling routine \n",
    "over multiple starter points and keys rather trivially.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(49)\n",
    "n_particles = 10_000\n",
    "starter_points = random.normal(key, shape=(n_particles, 1, 1)) * 5\n",
    "\n",
    "starter_keys = random.split(key, n_particles)\n",
    "\n",
    "trained_model_sampler = LangevinDynamicsChain(\n",
    "    gradient_func=updated_model, n_samples=100, epsilon=5e-1\n",
    ")\n",
    "final, trained_samples = vmap(trained_model_sampler)(starter_points, starter_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true \n",
    "#| fig-cap: (a) Initial and (b) final positions of points alongside (c) the original data.\n",
    "#| label: fig-langevin-dynamics-sampler-before-after\n",
    "fig, axes = plt.subplots(figsize=(8, 2.5), nrows=1, ncols=3, sharex=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plt.xlabel(\"Support\")\n",
    "plt.title(\"(a) Initial\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.hist(onp.array(starter_points.flatten()), bins=100)\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plt.xlabel(\"Support\")\n",
    "plt.title(\"(b) Final\")\n",
    "plt.hist(onp.array(final.flatten()), bins=100)\n",
    "\n",
    "plt.sca(axes[2])\n",
    "plt.hist(onp.array(data), bins=100)\n",
    "plt.title(\"(c) Original Data\")\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@fig-langevin-dynamics-sampler-before-after looks quite reasonable! \n",
    "Our original draws from a relatively wide Gaussian\n",
    "get split up into both component distribution which is encouraging here.\n",
    "This is encouraging!\n",
    "\n",
    "One thing I hope is evident here \n",
    "is the `vmap`-ing of the the sampler over multiple starting points.\n",
    "For me, that is one of the elegant things about JAX.\n",
    "With `vmap`, `lax.scan`, and other primitives in place,\n",
    "as long as we can \"stage out\" the elementary units of computation\n",
    "by implementing them as callables (or functions),\n",
    "we have a very clear path to incorporating them in loopy constructs\n",
    "such as `vmap` and `lax.scan`, and JIT-compiling them using `jit`."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "499aa38474d161e044ebb3be9240784e1719d4331ad512ef6546dcd230708004"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('score-models')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
